---
title: "Data Science: Inference and Modeling - HarvardX: PH125.4x"
output: html_notebook
---

# Section 6: Election Forecasting

## 6.1 Overview

In Section 6, you will learn about election forecasting, building on what you've learned in the previous sections about statistical modeling and Bayesian statistics.

After completing Section 6, you will be able to: 

* Understand how pollsters use hierarchical models to forecast the results of elections.
* Incorporate multiple sources of variability into a mathematical model to make predictions.
* Construct confidence intervals that better model deviations such as those seen in election data using the t-distribution.

```{r}
library(tidyverse)
```

## 6.2 Election Forecasting


### 6.2.1 Election Forecasting

**Key points**

* In our model:
  + The spread $d \sim N(\mu,\tau)$ describes our best guess in the absence of polling data. We set $\mu=0$ and $\tau=0.035$ using historical data.
  + The average of observed data $\bar{X} \mid d \sim N(d,\sigma)$ describes randomness due to sampling and the pollster effect.
* Because the posterior distribution is normal, we can report a 95% *credible interval* that has a 95% chance of overlapping the parameter using $E(p \mid Y)$ and $SE(p \mid Y)$.
* Given an estimate of$E(p \mid Y)$ and $SE(p \mid Y)$, we can use *pnorm* to compute the probability that $d>0$.
* It is common to see a general bias that affects all pollsters in the same way. This bias cannot be predicted or measured before the election. We will include a term in later models to account for this variability.

**Code: Definition of results object**

This code from previous videos defines the results object used for empirical Bayes election forecasting.

```{r}
library(tidyverse)
library(dslabs)
polls <- polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-10-31" &
                 (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

one_poll_per_pollster <- polls %>% group_by(pollster) %>%
    filter(enddate == max(enddate)) %>%
    ungroup()

results <- one_poll_per_pollster %>%
    summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
    mutate(start = avg - 1.96*se, end = avg + 1.96*se)
```

**Code: Computing the posterior mean, standard error, credible interval and probability**

Note that to compute an exact 95% credible interval, we would use qnorm(.975) instead of 1.96.
```{r}
mu <- 0
tau <- 0.035
sigma <- results$se
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)

posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt(1 / (1/sigma^2 + 1/tau^2))

posterior_mean
posterior_se

# 95% credible interval
posterior_mean + c(-1.96, 1.96) * posterior_se

# probability of d > 0
1 - pnorm(0, posterior_mean, posterior_se)
```


### 6.2.2 Mathematical Representations of Models

**Key points**

* If we collect several polls with measured spreads $X_1,...,X_j$ with a sample size of $N$, these random variables have expected value $d$ and standard error $2\sqrt{p(1-p)/N}$.
* We represent each measurement as $X_{i,j} = d+b+h_i+\epsilon_{i,j}$,where:
  + The index $i$ represents the different pollsters
  + The index $j$ represents the different polls
  + $X_{i,j}$ is the $j^{th}$ poll by the $i^{th}$ pollster 
  + $d$ is the actual spread of the election
  + $b$ is the general bias affecting all pollsters
  + $h_i$ represents the house effect for the $i^{th}$ pollster
  + $\epsilon_{i,j}$ represents the random error associated with the $i,j^{th}$ poll.
* The sample average is now $\bar{X} = d+b+\frac{1}{N}\sum_{i=1}^N X_i$ with standard deviation $SE(\bar{X}) = \sqrt{\sigma^2/N+\sigma_b^2}$.
* The standard error of the general bias $\sigma_b$ does not get reduced by averaging multiple polls, which increases the variability of our final estimate.

**Code: Simulated data with $X_j=d+\epsilon_j$ **
```{r}
J <- 6
N <- 2000
d <- .021
p <- (d+1)/2
X <- d + rnorm(J, 0, 2*sqrt(p*(1-p)/N))
```

**Code: Simulated data with $X_{i,j}=d+\epsilon_{i,j}$ **
```{r}
I <- 5
J <- 6
N <- 2000
d <- .021
p <- (d+1)/2
X <- sapply(1:I, function(i){
    d + rnorm(J, 0, 2*sqrt(p*(1-p)/N))
})
```

**Code: Simulated data with $X_{i,j}=d+h_i+\epsilon_{i,j}$ **
```{r}
I <- 5
J <- 6
N <- 2000
d <- .021
p <- (d+1)/2
h <- rnorm(I, 0, 0.025)    # assume standard error of pollster-to-pollster variability is 0.025
X <- sapply(1:I, function(i){
    d + rnorm(J, 0, 2*sqrt(p*(1-p)/N))
})
```

**Code: Calculating probability of $d>0$ with general bias**

Note that sigma now includes an estimate of the variability due to general bias $\sigma_b=.025$.
```{r}
mu <- 0
tau <- 0.035
sigma <- sqrt(results$se^2 + .025^2)
Y <- results$avg
B <- sigma^2 / (sigma^2 + tau^2)
posterior_mean <- B*mu + (1-B)*Y
posterior_se <- sqrt(1 / (1/sigma^2 + 1/tau^2))
1 - pnorm(0, posterior_mean, posterior_se)
```

### 6.2.3 Predicting the Electoral College

**Key points**

* In the US election, each state has a certain number of votes that are won all-or-nothing based on the popular vote result in that state (with minor exceptions not discussed here).
* We use the left_join function to combine the number of electoral votes with our poll results.
* For each state, we apply a Bayesian approach to generate an Election Day $d$. We keep our prior simple by assuming an expected value of 0 and a standard deviation based on recent history of 0.02.
* We can run a Monte Carlo simulation that for each iteration simulates poll results in each state using that state's average and standard deviation, awards electoral votes for each state to Clinton if the spread is greater than 0, then compares the number of electoral votes won to the number of votes required to win the election (over 269).
* If we run a Monte Carlo simulation for the electoral college without accounting for general bias, we overestimate Clinton's chances of winning at over 99%.
* If we include a general bias term, the estimated probability of Clinton winning decreases significantly.

**Code: Top 5 states ranked by electoral votes**

The results_us_election_2016 object is defined in the dslabs package:
```{r}
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
head(results_us_election_2016)
results_us_election_2016 %>% 
  arrange(desc(electoral_votes)) %>% 
  top_n(5, electoral_votes)
```

**Code: Computing the average and standard deviation for each state**
```{r}
results <- polls_us_election_2016 %>%
    filter(state != "U.S." &
            !grepl("CD", state) &
            enddate >= "2016-10-31" &
            (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
    group_by(state) %>%
    summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
    mutate(state = as.character(state))

# 10 closest races = battleground states
results %>% arrange(abs(avg))

# joining electoral college votes and results
results <- left_join(results, results_us_election_2016, by="state")

# states with no polls: note Rhode Island and District of Columbia = Democrat
results_us_election_2016 %>% filter(!state %in% results$state)

# assigns sd to states with just one poll as median of other sd values
results <- results %>%
    mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))
```

**Code: Calculating the posterior mean and posterior standard error**

Note there is a small error in the video code: B should be defined as $\sigma^2/(\sigma^2 + \tau^2)$.

```{r}
mu <- 0
tau <- 0.02
results %>% 
  mutate(sigma = sd/sqrt(n),
         B = sigma^2/ (sigma^2 + tau^2),
         posterior_mean = B*mu + (1-B)*avg,
         posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2))) %>%
  arrange(abs(posterior_mean))
```

**Code: Monte Carlo simulation of Election Night results (no general bias)**
```{r}
mu <- 0
tau <- 0.02
clinton_EV <- replicate(1000, {
    results %>% 
    mutate(sigma = sd/sqrt(n),
           B = sigma^2/ (sigma^2 + tau^2),
           posterior_mean = B*mu + (1-B)*avg,
           posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
           simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se), 
           clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    # award votes if Clinton wins state 
    summarize(clinton = sum(clinton)) %>%    # total votes for Clinton
    .$clinton + 7    # 7 votes for Rhode Island and DC
})
mean(clinton_EV > 269)    # over 269 votes wins election

# histogram of outcomes
data.frame(clinton_EV) %>%
    ggplot(aes(clinton_EV)) +
    geom_histogram(binwidth = 1) +
    geom_vline(xintercept = 269)
```

**Code: Monte Carlo simulation including general bias**
```{r}
mu <- 0
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
    results %>% 
    mutate(sigma = sqrt(sd^2/(n) + bias_sd^2),    # added bias_sd term
           B = sigma^2/ (sigma^2 + tau^2),
           posterior_mean = B*mu + (1-B)*avg,
           posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
           simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
           clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    # award votes if Clinton wins state
    summarize(clinton = sum(clinton)) %>%    # total votes for Clinton
    .$clinton + 7    # 7 votes for Rhode Island and DC
})
mean(clinton_EV_2 > 269)    # over 269 votes wins election
```


### 6.2.4 Forecasting

**Key points**

* In poll results, $p$ is not fixed over time. Variability within a single pollster comes from time variation.
* In order to forecast, our model must include a bias term $b_t$ to model the time effect.
* Pollsters also try to estimate $f(t)$, the trend of $p$ given time $t$ using a model like:
$$Y_{i,j} = d + b + h_j + b_t + f(t) + \epsilon_{i,j,t}$$
* Once we decide on a model, we can use historical data and current data to estimate the necessary parameters to make predictions.

**Code: Variability across one pollster**
```{r}
# select all national polls by one pollster
one_pollster <- polls_us_election_2016 %>%
    filter(pollster == "Ipsos" & state == "U.S.") %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# the observed standard error is higher than theory predicts
se <- one_pollster %>%
    summarize(empirical = sd(spread),
            theoretical = 2*sqrt(mean(spread)*(1-mean(spread))/min(samplesize)))
se

# the distribution of the data is not normal
one_pollster %>% ggplot(aes(spread)) +
    geom_histogram(binwidth = 0.01, color = "black")
```

**Code: Trend across time for several pollsters**
```{r}
polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-07-01") %>%
    group_by(pollster) %>%
    filter(n() >= 10) %>%
    ungroup() %>%
    mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
    ggplot(aes(enddate, spread)) +
    geom_smooth(method = "loess", span = 0.1) +
    geom_point(aes(color = pollster), show.legend = FALSE, alpha = 0.6)
```

**Code: Plotting raw percentages across time**
```{r}
polls_us_election_2016 %>%
    filter(state == "U.S." & enddate >= "2016-07-01") %>%
    select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%
    rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%
    gather(candidate, percentage, -enddate, -pollster) %>%
    mutate(candidate = factor(candidate, levels = c("Trump", "Clinton"))) %>%
    group_by(pollster) %>%
    filter(n() >= 10) %>%
    ungroup() %>%
    ggplot(aes(enddate, percentage, color = candidate)) +
    geom_point(show.legend = FALSE, alpha = 0.4) +
    geom_smooth(method = "loess", span = 0.15) +
    scale_y_continuous(limits = c(30, 50))
```

 
### 6.2.5 Assessment: Election Forecasting

#### Exercise 1 - Confidence Intervals of Polling Data
For each poll in the polling data set, use the CLT to create a 95% confidence interval for the spread. Create a new table called *cis* that contains columns for the lower and upper limits of the confidence intervals.

**Instructions**

* Use pipes *%>%* to pass the *poll* object on to the *mutate* function, which creates new variables.
* Create a variable called *X_hat* that contains the estimate of the proportion of Clinton voters for each poll.
* Create a variable called *se* that contains the standard error of the spread.
* Calculate the confidence intervals using the *qnorm* function and your calculated *se*.
* Use the *select* function to keep the following columns: *state, startdate, enddate, pollster, grade, spread, lower, upper*.

**Code**
```{r}
# Load the libraries and data
library(dplyr)
library(dslabs)
data("polls_us_election_2016")

# Create a table called `polls` that filters by state, date, and reports the spread
polls <- polls_us_election_2016 %>% 
  filter(state != "U.S." & enddate >= "2016-10-31") %>% 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

# Create an object called `cis` that has the columns indicated in the instructions
cis <- polls %>%
  mutate(X_hat = (spread+1)/2, 
         se = 2*sqrt(X_hat*(1-X_hat)/samplesize), 
         lower = spread - qnorm(.975) * se,
         upper = spread + qnorm(.975) * se) %>%
  select(state, startdate, enddate, pollster, grade, spread, lower, upper)
head(cis)
```

#### Exercise 2 - Compare to Actual Results 
You can add the final result to the *cis* table you just created using the *left_join* function as shown in the sample code.

Now determine how often the 95% confidence interval includes the actual result.

**Instructions**

* Create an object called *p_hits* that contains the proportion of intervals that contain the actual spread using the following steps.
* Use the *mutate* function to create a new variable called *hit* that contains a logical vector for whether the *actual_spread* falls between the *lower* and *upper* confidence intervals.
* Summarize the proportion of values in *hit* that are true as a variable called *proportion_hits*.

**Code**
```{r}
# Add the actual results to the `cis` data set
add <- results_us_election_2016 %>% 
  mutate(actual_spread = clinton/100 - trump/100) %>% 
  select(state, actual_spread)
ci_data <- cis %>% 
  mutate(state = as.character(state)) %>% 
  left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of confidence intervals that contain the actual value. Print this object to the console.
p_hits <- ci_data %>%
  mutate(hit = actual_spread >=lower & actual_spread <= upper) %>%
  summarize(mean(hit))
p_hits
```

#### Exercise 3 - Stratify by Pollster and Grade 
Now find the proportion of hits for each pollster. Show only pollsters with at least 5 polls and order them from best to worst. Show the number of polls conducted by each pollster and the FiveThirtyEight grade of each pollster.

**Instructions**

* Create an object called *p_hits* that contains the proportion of intervals that contain the actual spread using the following steps.
* Use the *mutate* function to create a new variable called *hit* that contains a logical vector for whether the *actual_spread* falls between the *lower* and *upper* confidence intervals.
* Use the *group_by* function to group the data by pollster.
* Use the *filter* function to filter for pollsters that have at least 5 polls.
* Summarize the proportion of values in *hit* that are true as a variable called *proportion_hits*. Also create new variables for the number of polls by each pollster (*n*) using the *n()* function and the grade of each poll (*grade*) by taking the first row of the grade column.
* Use the *arrange* function to arrange the *proportion_hits* in descending order.

**Code**
```{r}
# The `cis` data have already been loaded for you
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of hits for each pollster that has at least 5 polls.
p_hits <- ci_data %>%
  mutate(hit = actual_spread >=lower & actual_spread <= upper) %>%
  group_by(pollster) %>%
  filter(n()>=5) %>%
  summarize(proportion_hits = mean(hit), n=n(), grade=grade[1]) %>%
  arrange(desc(proportion_hits))
p_hits
```

#### Exercise 4 - Stratify by State 
Repeat the previous exercise, but instead of pollster, stratify by state. Here we can't show grades.

**Instructions**

* Create an object called *p_hits* that contains the proportion of intervals that contain the actual spread using the following steps.
* Use the *mutate* function to create a new variable called *hit* that contains a logical vector for whether the *actual_spread* falls between the *lower* and *upper* confidence intervals.
* Use the *group_by* function to group the data by pollster.
* Use the *filter* function to filter for pollsters that have at least 5 polls.
* Summarize the proportion of values in *hit* that are true as a variable called *proportion_hits*. Also create new variables for the number of polls by each pollster (*n*) using the *n()* function.
* Use the *arrange* function to arrange the *proportion_hits* in descending order.

**Code**
```{r}
# The `cis` data have already been loaded for you
add <- results_us_election_2016 %>% mutate(actual_spread = clinton/100 - trump/100) %>% select(state, actual_spread)
ci_data <- cis %>% mutate(state = as.character(state)) %>% left_join(add, by = "state")

# Create an object called `p_hits` that summarizes the proportion of hits for each state that has more than 5 polls.
p_hits <- ci_data %>%
  mutate(hit = actual_spread >=lower & actual_spread <= upper) %>%
  group_by(state) %>%
  filter(n()>=5) %>%
  summarize(proportion_hits = mean(hit), n=n()) %>%
  arrange(desc(proportion_hits))
p_hits
```

#### Exercise 5- Plotting Prediction Results 

Make a barplot based on the result from the previous exercise.

**Instructions**

* Reorder the states in order of the proportion of hits.
* Using ggplot, set the aesthetic with state as the x-variable and proportion of hits as the y-variable.
* Use geom_bar to indicate that we want to plot a barplot. Specifcy stat = "identity" to indicate that the height of the bar should match the value.
* Use coord_flip to flip the axes so the states are displayed from top to bottom and proportions are displayed from left to right.

**Code**
```{r}
# The `p_hits` data have already been loaded for you. Use the `head` function to examine it.
head(p_hits)

# Make a barplot of the proportion of hits for each state
p_hits %>% 
  mutate(state = reorder(state, proportion_hits)) %>%
  ggplot(aes(x=state, y=proportion_hits)) +
  geom_bar(stat="identity") +
  coord_flip()
```

#### Exercise 6 - Predicting the Winner 
Even if a forecaster's confidence interval is incorrect, the overall predictions will do better if they correctly called the right winner.

Add two columns to the cis table by computing, for each poll, the difference between the predicted spread and the actual spread, and define a column hit that is true if the signs are the same.

**Instructions**

* Use the mutate function to add two new variables to the cis object: error and hit.
* For the error variable, subtract the actual spread from the spread.
* For the hit variable, return "TRUE" if the poll predicted the actual winner.
* Save the new table as an object called errors.
* Use the tail function to examine the last 6 rows of errors.

**Code**
```{r}
# The `cis` data have already been loaded. Examine it using the `head` function.
head(ci_data)

# Create an object called `errors` that calculates the difference between the predicted and actual spread and indicates if the correct winner was predicted
errors <- ci_data %>%
  mutate(error = spread - actual_spread, hit = (sign(spread) == sign(actual_spread)))

# Examine the last 6 rows of `errors`
tail(errors, 6)
```

#### Exercise 7 - Plotting Prediction Results 
Create an object called p_hits that contains the proportion of instances when the sign of the actual spread matches the predicted spread for states with more than 5 polls.

Make a barplot based on the result from the previous exercise that shows the proportion of times the sign of the spread matched the actual result for the data in p_hits

**Instructions**

* Use the group_by function to group the data by state.
* Use the filter function to filter for states that have more than 5 polls.
* Summarize the proportion of values in hit that are true as a variable called proportion_hits. Also create new variables for the number of polls in each state using the n() function.
* To make the plot, follow these steps:
  + Reorder the states in order of the proportion of hits.
  + Using ggplot, set the aesthetic with state as the x-variable and proportion of hits as the y-variable.
  + Use geom_bar to indicate that we want to plot a barplot.
  + Use coord_flip to flip the axes so the states are displayed from top to bottom and proportions are displayed from left to right.

**Code**
```{r}
# Create an object called `errors` that calculates the difference between the predicted and actual spread and indicates if the correct winner was predicted
errors <- ci_data %>% mutate(error = spread - actual_spread, hit = sign(spread) == sign(actual_spread))

# Create an object called `p_hits` that summarizes the proportion of hits for each state that has more than 5 polls
p_hits <- errors %>%
  group_by(state) %>%
  filter(n()>5) %>%
  summarize(proportion_hits = mean(hit), n=n())

# Make a barplot of the proportion of hits for each state
p_hits %>% 
  mutate(state = reorder(state, proportion_hits)) %>%
  ggplot(aes(x=state, y=proportion_hits)) +
  geom_bar(stat="identity") +
  coord_flip()
```

#### Exercise 8 - Plotting the Errors 
In the previous graph, we see that most states' polls predicted the correct winner 100% of the time. Only a few states polls' were incorrect more than 25% of the time. Wisconsin got every single poll wrong. In Pennsylvania and Michigan, more than 90% of the polls had the signs wrong.

Make a histogram of the errors. What is the median of these errors?

**Instructions**

* Use the hist function to generate a histogram of the errors
* Use the median function to compute the median error

**Code**
```{r}
# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)

# Generate a histogram of the error
hist(errors$error)

# Calculate the median of the errors. Print this value to the console.
median(errors$error)
```

#### Exercise 9- Plot Bias by State 
We see that, at the state level, the median error was slightly in favor of Clinton. The distribution is not centered at 0, but at 0.037. This value represents the general bias we described in an earlier section.

Create a boxplot to examine if the bias was general to all states or if it affected some states differently. Filter the data to include only pollsters with grades B+ or higher.

**Instructions**

* Use the filter function to filter the data for polls with grades equal to A+, A, A-, or B+.
* Use the reorder function to order the state data by error.
* Using ggplot, set the aesthetic with state as the x-variable and error as the y-variable.
* Use geom_boxplot to indicate that we want to plot a boxplot.
* Use geom_point to add data points as a layer.

**Code**
```{r}
# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)

# Create a boxplot showing the errors by state for polls with grades B+ or higher
errors %>%
  filter(grade %in% c('A+', 'A', 'A-', 'B+')) %>%
  mutate(state = reorder(state, error)) %>%
  ggplot(aes(state, error)) + 
  geom_boxplot() +
  geom_point()
```

#### Exercise 10 - Filter Error Plot
Some of these states only have a few polls. Repeat the previous exercise to plot the errors for each state, but only include states with five good polls or more.

**Instructions**

* Use the filter function to filter the data for polls with grades equal to A+, A, A-, or B+.
* Group the filtered data by state using group_by.
* Use the filter function to filter the data for states with at least 5 polls.
* Use the reorder function to order the state data by error.
* Using ggplot, set the aesthetic with state as the x-variable and error as the y-variable.
* Use geom_box to indicate that we want to plot a boxplot.
* Use geom_point to add data points as a layer.

**Code**
```{r}
# The `errors` data have already been loaded. Examine them using the `head` function.
head(errors)

# Create a boxplot showing the errors by state for states with at least 5 polls with grades B+ or higher
errors %>%
  filter(grade %in% c('A+', 'A', 'A-', 'B+') & n()>5) %>%
  mutate(state = reorder(state, error)) %>%
  ggplot(aes(state, error)) + 
  geom_boxplot() +
  geom_point()
```


### 6.2.6 The t-Distribution

**Key points**

* In models where we must estimate two parameters, $p$ and $\sigma$, the Central Limit Theorem can result in overconfident confidence intervals for sample sizes smaller than approximately 30.
* If the population data are known to follow a normal distribution, theory tells us how much larger to make confidence intervals to account for estimation of $\sigma$.
* Given $s$ as an estimate of $\sigma$, then $Z = \frac{\bar{X}-d}{s/\sqrt{N}}$ follows a t-distribution with $N-1$ degrees of freedom.
* *Degrees of freedom* determine the weight of the tails of the distribution. Small values of *degrees of freedom* lead to increased probabilities of extreme values.
* We can determine confidence intervals using the *t-distribution* instead of the normal distribution by calculating the desired quantile with the function *qt*.

**Code: Calculating 95% confidence intervals with the t-distribution**
```{r}
z <- qt(0.975, nrow(one_poll_per_pollster) - 1)
one_poll_per_pollster %>%
    summarize(avg = mean(spread), moe = z*sd(spread)/sqrt(length(spread))) %>%
    mutate(start = avg - moe, end = avg + moe)

# quantile from t-distribution versus normal distribution
qt(0.975, 14)    # 14 = nrow(one_poll_per_pollster) - 1
qnorm(0.975)
```

### 6.2.7 Assessment: The t-Distribution

#### Exercise 1 - Using the t-Distribution
We know that, with a normal distribution, only 5% of values are more than 2 standard deviations away from the mean.

Calculate the probability of seeing t-distributed random variables being more than 2 in absolute value when the degrees of freedom are 3.

**Instructions**

* Use the pt function to calculate the probability of seeing a value less than or equal to the argument. Your output should be a single value.

**Code**
```{r}
# Calculate the probability of seeing t-distributed random variables being more than 2 in absolute value when 'df = 3'.
pt(-2,df=3) + 1-pt(2,df=3)
```

#### Exercise 2 - Plotting the t-distribution
Now use sapply to compute the same probability for degrees of freedom from 3 to 50.  
Make a plot and notice when this probability converges to the normal distribution's 5%.

**Instructions**

* Make a vector called df that contains a sequence of numbers from 3 to 50.
* Using function, make a function called pt_func that recreates the calculation for the probability that a value is greater than 2 as an absolute value for any given degrees of freedom.
* Use sapply to apply the pt_func function across all values contained in df. Call these probabilities probs.
* Use the plot function to plot df on the x-axis and probs on the y-axis.

**Code**
```{r}
# Generate a vector 'df' that contains a sequence of numbers from 3 to 50
df <- seq(3,50)

# Make a function called 'pt_func' that calculates the probability that a value is more than |2| for any degrees of freedom 
pt_func <- function(df) {
  pt(-2, df) + 1 - pt(2, df)
}

# Generate a vector 'probs' that uses the `pt_func` function to calculate the probabilities
probs <- sapply(df, pt_func)

# Plot 'df' on the x-axis and 'probs' on the y-axis
plot(df, probs)
```

#### Exercise 3 - Sampling From the Normal Distribution
In a previous section, we repeatedly took random samples of 50 heights from a distribution of heights. We noticed that about 95% of the samples had confidence intervals spanning the true population mean.

Re-do this Monte Carlo simulation, but now instead of N=50, use N=15. Notice what happens to the proportion of hits.

**Instructions**

* Use the replicate function to carry out the simulation. Specify the number of times you want the code to run and, within brackets, the three lines of code that should run.
* First use the sample function to randomly sample N values from x.
* Second, create a vector called interval that calculates the 95% confidence interval for the sample. You will use the qnorm function.
* Third, use the between function to determine if the population mean mu is contained between the confidence intervals.
* Save the results of the Monte Carlo function to a vector called res.
* Use the mean function to determine the proportion of hits in res.

**Code**
```{r}
# Load the neccessary libraries and data
library(dslabs)
library(dplyr)
data(heights)

# Use the sample code to generate 'x', a vector of male heights
x <- heights %>% filter(sex == "Male") %>%
  .$height

# Create variables for the mean height 'mu', the sample size 'N', and the number of times the simulation should run 'B'
mu <- mean(x)
N <- 15
B <- 10000

# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)

# Generate a logical vector 'res' that contains the results of the simulations
res <- replicate(B, {
  outcomes <- sample(x, N, replace=TRUE)
  interval <- mean(outcomes) + c(-qnorm(.975), qnorm(.975)) * sd(outcomes)/sqrt(N)
  between(mu,interval[1],interval[2])
})

# Calculate the proportion of times the simulation produced values within the 95% confidence interval. Print this value to the console.
mean(res)
```

#### Exercise 4 - Sampling from the t-Distribution
N=15  is not that big. We know that heights are normally distributed, so the t-distribution should apply. Repeat the previous Monte Carlo simulation using the t-distribution instead of using the normal distribution to construct the confidence intervals.

What are the proportion of 95% confidence intervals that span the actual mean height now?

**Instructions**

* Use the replicate function to carry out the simulation. Specify the number of times you want the code to run and, within brackets, the three lines of code that should run.
* First use the sample function to randomly sample N values from x.
* Second, create a vector called interval that calculates the 95% confidence interval for the sample. Remember to use the qt function this time to generate the confidence interval.
* Third, use the between function to determine if the population mean mu is contained between the confidence intervals.
* Save the results of the Monte Carlo function to a vector called res.
* Use the mean function to determine the proportion of hits in res.

**Code**
```{r}
# The vector of filtered heights 'x' has already been loaded for you. Calculate the mean.
mu <- mean(x)

# Use the same sampling parameters as in the previous exercise.
set.seed(1)
N <- 15
B <- 10000

# Generate a logical vector 'res' that contains the results of the simulations using the t-distribution
res <- replicate(B, {
  outcomes <- sample(x, N, replace=TRUE)
  interval <- mean(outcomes) + c(-qt(.975,14), qt(.975,14)) * sd(outcomes)/sqrt(N)
  between(mu,interval[1],interval[2])
})

# Calculate the proportion of times the simulation produced values within the 95% confidence interval. Print this value to the console.
mean(res)
```

#### Exercise 5 - Why the t-Distribution?
Why did the t-distribution confidence intervals work so much better?

**Instructions**  
A: The t-distribution takes the variability into account and generates larger confidence intervals.