---
title: 'Data Science: Inference and Modeling - HarvardX: PH125.4x'
output: html_notebook
---

# Section 7: Association Tests

## 7.1 Overview

In Section 7, you will learn how to use association and chi-squared tests to perform inference for binary, categorical, and ordinal data through an example looking at research funding rates.

After completing Section 7, you will be able to:

* Use association and chi-squared tests to perform inference on binary, categorical, and ordinal data.
* Calculate an odds ratio to get an idea of the magnitude of an observed effect.

```{r}
library(tidyverse)
```

## 7.2 Association Tests

### 7.2.1 Association Tests

**Key points**

* We learn how to determine the probability that an observation is due to random variability given categorical, binary or ordinal data.
* Fisher's exact test determines the *p-value* as the probability of observing an outcome as extreme or more extreme than the observed outcome given the null distribution.
* Data from a binary experiment are often summarized in *two-by-two tables*.
* The *p-value* can be calculated from a two-by-two table using Fisher's exact test with the function *fisher.test*. 

**Code: Research funding rates example**
```{r}
# load and inspect research funding rates object
library(tidyverse)
library(dslabs)
data(research_funding_rates)
research_funding_rates

# compute totals that were successful or not successful
totals <- research_funding_rates %>%
    select(-discipline) %>%
    summarize_all(funs(sum)) %>%
    summarize(yes_men = awards_men,
                         no_men = applications_men - awards_men,
                         yes_women = awards_women,
                         no_women = applications_women - awards_women)

# compare percentage of men/women with awards
totals %>% summarize(percent_men = yes_men/(yes_men + no_men),
                                          percent_women = yes_women/(yes_women + no_women))
```

**Code: Two-by-two table and p-value for the Lady Tasting Tea problem**
```{r}
tab <- matrix(c(3,1,1,3), 2, 2)
rownames(tab) <- c("Poured Before", "Poured After")
colnames(tab) <- c("Guessed Before", "Guessed After")
tab

# p-value calculation with Fisher's Exact Test
fisher.test(tab, alternative = "greater")
```

### 7.2.2 Chi-Squared Tests

**Key points**

* If the sums of the rows and the sums of the columns in the two-by-two table are fixed, then the hypergeometric distribution and  Fisher's exact test can be used. Otherwise, we must use the chi-squared test.
* The *chi-squared test* compares the observed two-by-two table to the two-by-two table expected by the null hypothesis and asks how likely it is that we see a deviation as large as observed or larger by chance.
* The function *chisq.test* takes a two-by-two table and returns the p-value from the chi-squared test.
* The *odds ratio* states how many times larger the odds of an outcome are for one group relative to another group.
* A small *p-value* does not imply a large *odds ratio*. If a finding has a small p-value but also a small odds ratio, it may not be a practically significant or scientifically significant finding. 
* Because the *odds ratio* is a ratio of ratios, there is no simple way to use the Central Limit Theorem to compute confidence intervals. There are advanced methods for computing confidence intervals for odds ratios that we do not discuss here.

**Code: Chi-squared test**
```{r}
# compute overall funding rate
funding_rate <- totals %>%
    summarize(percent_total = (yes_men + yes_women) / (yes_men + no_men + yes_women + no_women)) %>%
    .$percent_total
funding_rate

# construct two-by-two table for observed data
two_by_two <- tibble(awarded = c("no", "yes"),
                                      men = c(totals$no_men, totals$yes_men),
                                      women = c(totals$no_women, totals$yes_women))
two_by_two

# compute null hypothesis two-by-two table
tibble(awarded = c("no", "yes"),
           men = (totals$no_men + totals$yes_men) * c(1-funding_rate, funding_rate),
           women = (totals$no_women + totals$yes_women) * c(1-funding_rate, funding_rate))

# chi-squared test
chisq_test <- two_by_two %>%
    select(-awarded) %>%
    chisq.test()
chisq_test$p.value
```

**Code: Odds ratio**
```{r}
# odds of getting funding for men
odds_men <- (two_by_two$men[2] / sum(two_by_two$men)) /
        (two_by_two$men[1] / sum(two_by_two$men))

# odds of getting funding for women
odds_women <- (two_by_two$women[2] / sum(two_by_two$women)) /
        (two_by_two$women[1] / sum(two_by_two$women))

# odds ratio - how many times larger odds are for men than women
odds_men/odds_women
```

**Code: p-value and odds ratio responses to increasing sample size**
```{r}
# multiplying all observations by 10 decreases p-value without changing odds ratio
two_by_two %>%
 select(-awarded) %>%
 mutate(men = men*10, women = women*10) %>%
 chisq.test()
```


### 7.2.3 Assessment: Association and Chi-Squared Tests

#### Exercise 1 - Comparing Proportions of Hits
In a previous exercise, we determined whether or not each poll predicted the correct winner for their state in the 2016 U.S. presidential election. Each poll was also assigned a grade by the poll aggregator. Now we're going to determine if polls rated A- made better predictions than polls rated C-.

In this exercise, filter the errors data for just polls with grades A- and C-. Calculate the proportion of times each grade of poll predicted the correct winner.

**Instructions**

* Filter *errors* for grades A- and C-.
* Group the data by grade and hit.
* Summarize the number of hits for each grade.
* Generate a two-by-two table containing the number of hits and misses for each grade.
* Calculate the proportion of times each grade was correct.

**Code**
```{r}
# The 'errors' data have already been loaded. Examine them using the `head` function.
head(errors)

# Generate an object called 'totals' that contains the numbers of good and bad predictions for polls rated A- and C-
res <- errors %>%
  filter(grade %in% c('A-','C-')) %>%
  group_by(grade, hit) %>%
  summarize(total_hits=n())
totals <- tibble(predictions = c("good", "bad"),
                 'A-' = c(res[res$grade=='A-' & res$hit==TRUE,]$total_hits, res[res$grade=='A-' & res$hit==FALSE,]$total_hits),
                 'C-' = c(res[res$grade=='C-' & res$hit==TRUE,]$total_hits, res[res$grade=='C-' & res$hit==FALSE,]$total_hits))
totals

# Print the proportion of hits for grade A- polls to the console
totals$'A-'[1] / sum(totals$'A-')

# Print the proportion of hits for grade C- polls to the console
totals$'C-'[1] / sum(totals$'C-')
```

#### Exercise 2 - Chi-squared Test
We found that the A- polls predicted the correct winner about 86% of the time in their states and C- polls predicted the correct winner about 80% of the time.

Use a chi-squared test to determine if these proportions are different.

**Instructions**

* Use the *chisq.test* function to perform the chi-squared test. Save the results to an object called *chisq_test*.
* Print the *p-value* of the test to the console.

**Code**
```{r}
# The 'totals' data have already been loaded. Examine them using the `head` function.
head(totals)

# Perform a chi-squared test on the hit data. Save the results as an object called 'chisq_test'.
chisq_test <- totals %>%
  select(-predictions) %>%
  chisq.test()

# Print the p-value of the chi-squared test to the console
chisq_test$p.value
```

#### Exercise 3 - Odds Ratio Calculation
It doesn't look like the grade A- polls performed significantly differently than the grade C- polls in their states.

Calculate the odds ratio to determine the magnitude of the difference in performance between these two grades of polls.

**Instructions**

* Calculate the odds that a grade C- poll predicts the correct winner. Save this result to a variable called *odds_C*.
* Calculate the odds that a grade A- poll predicts the correct winner. Save this result to a variable called *odds_A*.
* Calculate the odds ratio that tells us how many times larger the odds of a grade A- poll is at predicting the winner than a grade C- poll.

**Code**
```{r}
# The 'totals' data have already been loaded. Examine them using the `head` function.
head(totals)

# Generate a variable called `odds_C` that contains the odds of getting the prediction right for grade C- polls
odds_C <- totals$'C-'[1] / totals$'C-'[2] 
  
# Generate a variable called `odds_A` that contains the odds of getting the prediction right for grade A- polls
odds_A <- totals$'A-'[1] / totals$'A-'[2] 

# Calculate the odds ratio to determine how many times larger the odds ratio is for grade A- polls than grade C- polls
odds_A / odds_C
```

#### Exercise 4 - Significance
We did not find meaningful differences between the poll results from grade A- and grade C- polls in this subset of the data, which only contains polls for about a week before the election. Imagine we expanded our analysis to include all election polls and we repeat our analysis. In this hypothetical scenario, we get that the p-value for the difference in prediction success if 0.0015 and the odds ratio describing the effect size of the performance of grade A- over grade B- polls is 1.07.

Based on what we learned in the last section, which statement reflects the best interpretation of this result?

**Answer:** The p-value is below 0.05, but the odds ratio is very close to 1. There is not a scientifically significant difference in performance.