---
title: 'Data Science: Linear Regression - HarvardX: PH125.7x'
author: 'Luiz Cunha'
date: '2019-08-13'
output: html_notebook
---

# Introduction and Welcome

## Welcome to Data Science: Linear Regression!

### Welcome to Data Science: Linear Regression!

Welcome to *Data Science: Linear Regression!* We're excited to have you join us in this course, which is designed to teach you about linear regression, one of the most common statistical modeling approaches used in data science.

This is the seventh course in the HarvardX Professional Certificate in Data Science, a series of courses that prepare you to do data analysis in R, from simple computations to machine learning. We assume that you have either taken the preceding courses in the series or that you are already familiar with the content covered in them.

Linear regression is commonly used to quantify the relationship between two or more variables. It is also used to adjust for confounding. In this course, we cover how to implement linear regression and adjust for confounding in practice using R.

In data science applications, it is very common to be interested in the relationship between two or more variables. The motivating case study we examine in this course relates to the data-driven approach used to construct baseball teams described in the book (and movie) Moneyball. We will try to determine which measured outcomes best predict baseball runs and to do this we'll use linear regression. 

We will also examine confounding, where extraneous variables affect the relationship between two or more other variables, leading to spurious associations. Linear regression is a powerful technique for removing confounders, but it is not a magical process, and it is essential to understand when it is appropriate to use. You will learn when to use it in this course. 

The class notes for this course series can be found in Professor Irizarry's freely available Introduction to Data Science book.

**In this course, you will learn:**

How linear regression was originally developed by Galton
What confounding is and how to detect it
How to examine the relationships between variables by implementing linear regression in R

#### Course overview

There are three major sections in this course: introduction to linear regression, linear models, and confounding.

**Introduction to Linear Regression**

In this section, you'll learn the basics of linear regression through this course's motivating example, the data-driven approach used to construct baseball teams. You'll also learn about correlation, the correlation coefficient, stratification, and the variance explained.

**Linear Models**

In this section, you'll learn about linear models. You'll learn about least squares estimates, multivariate regression, and several useful features of R, such as *tibbles, lm, do, and broom*. You'll learn how to apply regression to baseball to build a better offensive metric.

**Confounding**

In the final section of the course, you'll learn about confounding and several reasons that correlation is not the same as causation, such as spurious correlation, outliers, reversing cause and effect, and confounders. You'll also learn about Simpson's Paradox.


# Section 1: Introduction to Regression

## Overview

In the **Introduction to Regression** section, you will learn the basics of linear regression.

After completing this section, you will be able to:

* Understand how Galton developed **linear regression**.
* Calculate and interpret the **sample correlation**.
* **Stratify** a dataset when appropriate.
* Understand what a **bivariate normal distribution** is.
* Explain what the term **variance explained** means.
* Interpret the two **regression lines**.

This section has three parts: **Baseball as a Motivating Example**, **Correlation**, and **Stratification and Variance Explained**.


## 1.1 Baseball as a Motivation Example

### 1.1.1 Motivating Example: Moneyball

**Key points**

* Bill James was the originator of the **sabermetrics**, the approach of using data to predict what outcomes best predicted if a team would win.

### 1.1.2 Baseball Basics

**Key points**

* The goal of a baseball game is to score more runs (points) than the other team.
* Each team has 9 batters who have an opportunity to hit a ball with a bat in a predetermined order. 
* Each time a batter has an opportunity to bat, we call it a plate appearance (PA).
* The PA ends with a binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn???t (success) and can run around the bases, and potentially score a run (reach all 4 bases).
* We are simplifying a bit, but there are five ways a batter can succeed (not make an out):
  1. Bases on balls (BB): the pitcher fails to throw the ball through a predefined area considered to be hittable (the strike zone), so the batter is permitted to go to first base.
  2. Single: the batter hits the ball and gets to first base.
  3. Double (2B): the batter hits the ball and gets to second base.
  4. Triple (3B): the batter hits the ball and gets to third base.
  5. Home Run (HR): the batter hits the ball and goes all the way home and scores a run.
* Historically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples and home runs are hits. The fifth way to be successful, a walk (BB), is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate.


### 1.1.3 Bases on Balls or Stolen Bases?

**Key points**

* The visualization of choice when exploring the relationship between two variables like home runs and runs is a scatterplot.

**Code**

Scatterplot of the relationship between HRs and runs
```{r}
library(Lahman)
library(tidyverse)
library(dslabs)
ds_theme_set()

Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(HR_per_game = HR / G, R_per_game = R / G) %>%
    ggplot(aes(HR_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```


Scatterplot of the relationship between stolen bases and runs
```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(SB_per_game = SB / G, R_per_game = R / G) %>%
    ggplot(aes(SB_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

Scatterplot of the relationship between bases on balls and runs
```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(BB_per_game = BB / G, R_per_game = R / G) %>%
    ggplot(aes(BB_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

Scatterplot of the relationship between at bats and runs
```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(AB_per_game = AB / G, R_per_game = R / G) %>%
    ggplot(aes(AB_per_game, R_per_game)) + 
    geom_point(alpha = 0.5)
```

Scatterplot of the relationship between fielding errors and wins
```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(win_rate = W / G, E_per_game = E / G) %>%
  ggplot(aes(win_rate, E_per_game)) + 
  geom_point(alpha = 0.5)
```

## 1.2 Correlation

### 1.2.1 Correlation

**Key points**

* Galton tried to predict sons' heights based on fathers' heights.
* The mean and standard errors are insufficient for describing an important characteristic of the data: the trend that the taller the father, the taller the son.
* The correlation coefficient is an informative summary of how two variables move together that can be used to predict one variable using the other.

**Code**
```{r}
# create the dataset
library(tidyverse)
library(HistData)
data("GaltonFamilies")
set.seed(1983)
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)

# means and standard deviations
galton_heights %>%
    summarize(mean(father), sd(father), mean(son), sd(son))

# scatterplot of father and son heights
galton_heights %>%
    ggplot(aes(father, son)) +
    geom_point(alpha = 0.5)
```



### 1.2.2 Correlation Coefficient

**Key points**

* The correlation coefficient is defined for a list of pairs $(x_1, y_1), ..., (x_n, y_n)$ as the product of the standardized values: $\frac{x_i-\mu_x}{\sigma_x}*  \frac{y_i-\mu_y}{\sigma_y}$.
* The correlation coefficient essentially conveys how two variables move together.
* The correlation coefficient is always between -1 and 1.

**Code**

```{r}
rho <- function(x,y) mean(scale(x)*scale(y))
galton_heights %>% summarize(r = cor(father, son)) %>% pull(r)
```

### 1.2.3 Sample Correlation is a Random Variable

**Key points**

* The correlation that we compute and use as a summary is a random variable.
* When interpreting correlations, it is important to remember that correlations derived from samples are estimates containing uncertainty.
* Because the sample correlation is an average of independent draws, the central limit theorem applies. 

**Code**
```{r}
# compute sample correlation
R <- sample_n(galton_heights, 25, replace = TRUE) %>%
    summarize(r = cor(father, son))
R
# Monte Carlo simulation to show distribution of sample correlation
B <- 1000
N <- 25
R <- replicate(B, {
    sample_n(galton_heights, N, replace = TRUE) %>%
    summarize(r = cor(father, son)) %>%
    pull(r)
})
qplot(R, geom = "histogram", binwidth = 0.05, color = I("black"))

# expected value and standard error
mean(R)
sd(R)
# QQ-plot to evaluate whether N is large enough
data.frame(R) %>%
    ggplot(aes(sample = R)) +
    stat_qq() +
    geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))
```

### 1.2.4 Assessment: Correlation

```{r}
Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(AB_per_game = AB / G, R_per_game = R / G) %>%
    summarize(cor(AB_per_game, R_per_game))

Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(W_per_game = W / G, E_per_game = E / G) %>%
    summarize(cor(W_per_game, E_per_game))

Teams %>% filter(yearID %in% 1961:2001) %>%
    mutate(X2B_per_game = X2B / G, X3B_per_game = X3B / G) %>%
    summarize(cor(X2B_per_game, X3B_per_game))
```


## 1.3 Stratification and Variance Explained

### 1.3.1 Anscombe's Quartet/Stratification

**Key points**

* Correlation is not always a good summary of the relationship between two variables.
* The general idea of conditional expectation is that we stratify a population into groups and compute summaries in each group.
* A practical way to improve the estimates of the conditional expectations is to define strata of with similar values of x.
* If there is perfect correlation, the regression line predicts an increase that is the same number of SDs for both variables. If there is 0 correlation, then we don???t use x at all for the prediction and simply predict the average  ???????? . For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.

**Code**
```{r}
# number of fathers with height 72 or 72.5 inches
sum(galton_heights$father == 72)
sum(galton_heights$father == 72.5)

# predicted height of a son with a 72 inch tall father
conditional_avg <- galton_heights %>%
    filter(round(father) == 72) %>%
    summarize(avg = mean(son)) %>%
    pull(avg)
conditional_avg

# stratify fathers' heights to make a boxplot of son heights
galton_heights %>% mutate(father_strata = factor(round(father))) %>%
    ggplot(aes(father_strata, son)) +
    geom_boxplot() +
    geom_point()

# center of each boxplot
galton_heights %>%
    mutate(father = round(father)) %>%
    group_by(father) %>%
    summarize(son_conditional_avg = mean(son)) %>%
    ggplot(aes(father, son_conditional_avg)) +
    geom_point()

# calculate values to plot regression line on original data
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)
m <- r * s_y/s_x
b <- mu_y - m*mu_x

# add regression line to plot
galton_heights %>%
    ggplot(aes(father, son)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = b, slope = m)
```


### 1.3.2 Bivariate Normal Distribution

**Key points**

* When a pair of random variables are approximated by the bivariate normal distribution, scatterplots look like ovals. They can be thin (high correlation) or circle-shaped (no correlation).
* When two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations.
* We can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to make predictions.

**Code**
```{r}
galton_heights %>%
  mutate(z_father = round((father - mean(father)) / sd(father))) %>%
  filter(z_father %in% -2:2) %>%
  ggplot() +  
  stat_qq(aes(sample = son)) +
  facet_wrap( ~ z_father)
```


### 1.3.3 Variance Explained

**Key points**

* Conditioning on a random variable X can help to reduce variance of response variable Y.
* The standard deviation of the conditional distribution is $SD(Y \mid X=x) = \sigma_y \sqrt{1-\rho^2}$, which is smaller than the standard deviation without conditioning $\sigma_y$.
* Because variance is the standard deviation squared, the variance of the conditional distribution is $\sigma_y^2 (1-\rho^2)$.
* In the statement "X explains such and such percent of the variability," the percent value refers to the variance. The variance decreases by $\rho^2$ percent.
* The *variance explained* statement only makes sense when the data is approximated by a bivariate normal distribution.


### 1.3.4 There are Two Regression Lines

**Key point**

* There are two different regression lines depending on whether we are taking the expectation of Y given X or taking the expectation of X given Y.

**Code**
```{r}
# compute a regression line to predict the son's height from the father's height
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)
m_1 <-  r * s_y / s_x
b_1 <- mu_y - m_1*mu_x

# compute a regression line to predict the father's height from the son's height
m_2 <-  r * s_x / s_y
b_2 <- mu_x - m_2*mu_y
```

### Assessment: Stratification and Variance Explained, Part 1
...

### Assessment: Stratification and Variance Explained, Part 2

In the second part of this assessment, you'll analyze a set of mother and daughter heights, also from GaltonFamilies.

Define female_heights, a set of mother and daughter heights sampled from GaltonFamilies, as follows:

```{r}
options(digits=3)
set.seed(1989) #if you are using R 3.5 or earlier
library(HistData)
data("GaltonFamilies")

female_heights <- GaltonFamilies%>%     
    filter(gender == "female") %>%     
    group_by(family) %>%     
    sample_n(1) %>%     
    ungroup() %>%     
    select(mother, childHeight) %>%     
    rename(daughter = childHeight)
```

#### Question 8
Calculate the mean and standard deviation of mothers' heights, the mean and standard deviation of daughters' heights, and the correlaton coefficient between mother and daughter heights.

```{r}
mu_m <- mean(female_heights$mother)
mu_m
sigma_m <- sd(female_heights$mother)

mu_d <- mean(female_heights$daughter)
sigma_d <- sd(female_heights$daughter)

rho <- cor(female_heights$mother, female_heights$daughter)
cor(female_heights$daughter, female_heights$mother)
```

#### Question 9
Calculate the slope and intercept of the regression line predicting daughters' heights given mothers' heights. Given an increase in mother's height by 1 inch, how many inches is the daughter's height expected to change?

```{r}
# Slope of regression line predicting daughters' height from mothers' heights
rho * sigma_d / sigma_m

#Intercept of regression line predicting daughters' height from mothers' heights
mu_d - rho * sigma_d / sigma_m * mu_m

# Change in daughter's height in inches given a 1 inch increase in the mother's height
rho * sigma_d / sigma_m
```

#### Question 10
What percent of the variability in daughter heights is explained by the mother's height?
A: $\rho^2 * 100$

#### Question 11
A mother has a height of 60 inches.
What is the conditional expected value of her daughter's height given the mother's height?
A: 

```{r}
x_ <- (60 - mu_m) / sigma_m
y <- mu_d + rho* sigma_d * x_
y
```

