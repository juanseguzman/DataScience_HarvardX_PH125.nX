---
title: 'Data Science: Machine Learning - HarvardX: PH125.8x'
author: 'Luiz Cunha'
date: '2019-08-15'
output: html_notebook
---

```{r include=FALSE}
# Set knitr options for knitting code into the report:
# - echo=TRUE: print out code (echo), though any results/output would still be displayed
# - cache=TRUE: Save results so that code blocks aren't re-run unless code changes (cache),
# _or_ a relevant earlier code block changed (autodep), but don't re-run if the
# only thing that changed was the comments (cache.comments)
# - message, warning = FALSE: Don't clutter R output with messages or warnings
# This _will_ leave error messages showing up in the knitted report
# - results="hide": hide the results/output (but here the code would still be displayed)
# - include=FALSE:  chunk is evaluated, but neither the code nor its output is displayed
# - eval=FALSE: chunk is not evaluated
knitr::opts_chunk$set(echo=TRUE,
               cache=TRUE, autodep=TRUE, cache.comments=FALSE,
               message=FALSE, warning=FALSE)
```

# Section 3: Linear Regression for Prediction, Smoothing, and Working with Matrices Overview

##  Overview

In the **Linear Regression for Prediction, Smoothing, and Working with Matrices Overview** section, you will learn why linear regression is a useful baseline approach but is often insufficiently flexible for more complex analyses, how to smooth noisy data, and how to use matrices for machine learning.

After completing this section, you will be able to:

* Use **linear regression for prediction** as a baseline approach.
* Use **logistic regression** for categorical data.
* Detect trends in noisy data using **smoothing** (also known as **curve fitting** or **low pass filtering**).
* Convert predictors to **matrices** and outcomes to **vectors** when all predictors are numeric (or can be converted to numerics in a meaningful way).
* Perform basic **matrix algebra** calculations.

This section has three parts: **linear regression for prediction**, **smoothing** and **working with matrices**. 


## 3.1 Linear Regression for Prediction

### 3.1.1 Linear Regression for Prediction

**Key points**

* Linear regression can be considered a machine learning algorithm. Although it can be too rigid to be useful, it works rather well for some challenges. It also serves as a baseline approach: if you cant beat it with a more complex approach, you probably want to stick to linear regression. 

**Code**
```{r}
library(tidyverse)
library(caret)
library(HistData)
set.seed(1983, sample.kind="Rounding")

galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)

y <- galton_heights$son
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- galton_heights %>% slice(-test_index)
test_set <- galton_heights %>% slice(test_index)

m <- mean(train_set$son)

# squared loss
mean((m - test_set$son)^2)

# fit linear regression model
fit <- lm(son ~ father, data = train_set)
fit$coef
y_hat <- fit$coef[1] + fit$coef[2]*test_set$father
mean((y_hat - test_set$son)^2)
```

### 3.1.2 The predict() Function

**Key points**

* The **predict** function takes a fitted object from functions such as **lm** or **glm** and a data frame with the new predictors for which to predict. We can use predict like this: *y_hat <- predict(fit, test_set)*
* **predict** is a generic function in R that calls other functions depending on what kind of object it receives. To learn about the specifics, you can read the help files using code like this: *?predict.lm* or *?predict.glm*

**Code**
```{r}
y_hat <- predict(fit, test_set)
mean((y_hat - test_set$son)^2)

# read help files
?predict.lm
?predict.glm
```

### 3.1.3 Comprehension Check: Linear Regression

#### Question 1
We will build 100 linear models using the data above and calculate the mean and standard deviation of the combined models.  

* First, set the seed to 1 again (make sure to use sample.kind="Rounding" if your R is version 3.6 or later).  
* Then, within a replicate loop, 
  + (1) partition the dataset into test and training sets of equal size using dat$y to generate your indices, 
  + (2) train a linear model predicting y from x, 
  * (3) generate predictions on the test set, and 
  * (4) calculate the RMSE of that model. 
* Then, report the mean and standard deviation (SD) of the RMSEs from all 100 models.

Create a data set using the following code:
```{r}
options(digits=3)
set.seed(1, sample.kind="Rounding") 
# set.seed(1, sample.kind="Rounding") if using R 3.6 or later
n <- 100
Sigma <- 9 * matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat1 <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
      data.frame() %>% setNames(c("x", "y"))
```

```{r}
set.seed(1, sample.kind="Rounding") 

RMSE <- replicate(100, {
    test_index <- createDataPartition(dat1$y, times = 1, p = 0.5, list = FALSE)
    train <- dat1 %>% slice(-test_index)
    test <- dat1 %>% slice(test_index)
    mod <- lm(y ~ x, data=train)
    y_hat <- predict(mod, test)
    sqrt(mean((y_hat - test$y)^2))
})
mean(RMSE)
sd(RMSE)
```


#### Question 2
Now we will repeat the exercise above but using larger datasets.  
Write a function that takes a size n, then:

* (1) builds a dataset using the code provided in Q1 but with n observations instead of 100 and without the set.seed(1), 
* (2) runs the replicate loop that you wrote to answer Q1, which builds 100 linear models and returns a vector of RMSEs, and 
* (3) calculates the mean and standard deviation.
* Set the seed to 1 (if using R 3.6 or later, use the argument sample.kind="Rounding") and then 
* use sapply or map to apply your new function to n <- c(100, 500, 1000, 5000, 10000).

Hint: You only need to set the seed once before running your function; do not set a seed within your function. Also be sure to use sapply or map as you will get different answers running the simulations individually due to setting the seed.

```{r}
options(digits=3)
set.seed(1, sample.kind="Rounding")

n <- c(100, 500, 1000, 5000, 10000)
res2 <- sapply(n, function(n) {
  Sigma <- 9 * matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
  dat_n <- MASS::mvrnorm(n, c(69, 69), Sigma) %>%
    data.frame() %>% setNames(c("x", "y"))

  RMSE_n <- replicate(100, {
    test_index <- createDataPartition(dat_n$y, times = 1, 
                                      p = 0.5, list = FALSE)
    train <- dat_n %>% slice(-test_index)
    test <- dat_n %>% slice(test_index)
    mod <- lm(y ~ x, data = train)
    y_hat <- predict(mod, test)
    sqrt(mean((y_hat - test$y)^2))
    })
  c(mu = mean(RMSE_n), sigma = sd(RMSE_n))
})

res2
```

#### Question 3
Q: What happens to the RMSE as the size of the dataset becomes larger?  
A: On average, the RMSE does not change much as n gets larger, but the variability of the RMSE decreases. 

#### Question 4
Now repeat the exercise from Q1, this time making the correlation between x and y larger, as in the following code:

Note what happens to RMSE - set the seed to 1 as before.

```{r}
set.seed(1, sample.kind="Rounding")
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat4 <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))

set.seed(1, sample.kind="Rounding")
RMSE <- replicate(100, {
    test_index <- createDataPartition(dat4$y, times = 1, p = 0.5, list = FALSE)
    train <- dat4 %>% slice(-test_index)
    test <- dat4 %>% slice(test_index)
    fit <- lm(y ~ x, data=train)
    y_hat <- predict(fit, test)
    sqrt(mean((y_hat - test$y)^2))
})
c(mu = mean(RMSE), sigma= sd(RMSE))
```

#### Question 5
Q: Which of the following best explains why the RMSE in question 4 is so much lower than the RMSE in question 1?
A: When we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y.

#### Question 6
Create a data set using the following code.

```{r}
set.seed(1, sample.kind="Rounding")
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat6 <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
cor(dat6)
```

```{r}
library(tidyverse)
library(caret)
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(dat6$y, times = 1, p = 0.5, list = FALSE)
train <- dat6 %>% slice(-test_index)
test <- dat6 %>% slice(test_index)

fit <- lm(y ~ x_1, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))

fit <- lm(y ~ x_2, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))

fit <- lm(y ~ x_1+x_2, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))
```

Note that y is correlated with both x_1 and x_2 but the two predictors are independent of each other, as seen by cor(dat).

Set the seed to 1, then use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2 and both x_1 and x_2. Train a linear model for each.

Q: Which of the three models performs the best (has the lowest RMSE)?  
A: fit <- lm(y ~ x_1+x_2, data=train)

#### Question 8
Repeat the exercise from Q6 but now create an example in which x_1 and x_2 are highly correlated.

```{r}
set.seed(1, sample.kind="Rounding")
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
dat8 <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
```

Set the seed to 1, then use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2.

```{r}
library(tidyverse)
library(caret)
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(dat8$y, times = 1, p = 0.5, list = FALSE)
train <- dat8 %>% slice(-test_index)
test <- dat8 %>% slice(test_index)

fit <- lm(y ~ x_1, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))

fit <- lm(y ~ x_2, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))

fit <- lm(y ~ x_1+x_2, data=train)
y_hat <- predict(fit, test)
sqrt(mean((y_hat - test$y)^2))
```

Q: Compare the results from Q6 and Q8. What can you conclude?  
A: Adding extra predictors can improve RMSE substantially, but not when the added predictors are highly correlated with other predictors.


### 3.1.4 Regression for a Categorical Outcome

**Key points**

* The regression approach can be extended to categorical data. For example, we can try regression to estimate the conditional probability:

$$p(x) = Pr(Y=1 \mid X=x) = \beta_0 + \beta_1 x$$

* Once we have estimates $beta_0$ and $beta_1$, we can obtain an actual prediction p(x). Then we can define a specific decision rule to form a prediction.

**Code**
```{r}
library(dslabs)
data("heights")
y <- heights$height

set.seed(2, sample.kind="Rounding")
#set.seed(2, sample.kind = "Rounding") #if you are using R 3.6 or later

test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)

train_set %>% 
  filter(round(height)==66) %>%
  summarize(y_hat = mean(sex=="Female"))

heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female")) %>%
  ggplot(aes(x, prop)) +
  geom_point()

lm_fit <- mutate(train_set, y = as.numeric(sex == "Female")) %>% lm(y ~ height, data = .)
p_hat <- predict(lm_fit, test_set)
y_hat <- ifelse(p_hat > 0.5, "Female", "Male") %>% factor()
confusionMatrix(y_hat, test_set$sex)$overall["Accuracy"]
```

### 3.1.5 Logistic Regression

**Key points**

* **Logistic regression** is an extension of linear regression that assures that the estimate of conditional probability $Pr(Y=1 \mid X=x)$ is between 0 and 1. This approach makes use of the logistic transformation: 

$$g(p) = \log\frac{p}{1-p}$$

* With logistic regression, we model the conditional probability directly with:

$$g(Pr(Y=1 \mid X=x)) = \beta_0 + \beta_1 x$$

* Note that with this model, we can no longer use least squares. Instead we compute the **maximum likelihood estimate (MLE)**.
* In R, we can fit the logistic regression model with the function **glm (generalized linear models)**: *eg. glm(formula..., data=..., family="binomial")*
* If we want to compute the conditional probabilities, we want **type="response"** since the default is to return the logistic transformed values: *eg. predict(glm_fitted..., newdata=..., type = "response")*  

**Code**
```{r}
heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female")) %>%
  ggplot(aes(x, prop)) +
  geom_point() + 
  geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])
range(p_hat)

# fit logistic regression model
glm_fit <- train_set %>% 
  mutate(y = as.numeric(sex == "Female")) %>%
  glm(y ~ height, data=., family = "binomial")
p_hat_logit <- predict(glm_fit, newdata = test_set, type = "response")
y_hat_logit <- ifelse(p_hat_logit > 0.5, "Female", "Male") %>% factor
confusionMatrix(y_hat_logit, test_set$sex)[["Accuracy"]]
```


### 3.1.6 Case Study: "2" or "7"

**Key points**

* In this case study we apply logistic regression to classify whether a digit is two or seven. We are interested in estimating a conditional probability that depends on two variables:

$$g(p(x_1, x_2)) = g(Pr(Y=1 \mid X_1=x_1, X_2=x_2)) = \beta_0 + \beta_1 x$$

* Through this case, we know that logistic regression forces our estimates to be a **plane** and our boundary to be a **line**. This implies that a logistic regression approach has no chance of capturing the **non-linear** nature of the true $p(x_1,x_2)$. Therefore, we need other more flexible methods that permit other shapes.

**Code**
```{r}
mnist <- read_mnist()
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_1),        
                                                                which.max(mnist_27$train$x_1))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
    expand.grid(Row=1:28, Column=1:28) %>%
        mutate(label=titles[i],
               value = mnist$train$images[is[i],])
})
tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) +
    geom_raster() +
    scale_y_reverse() +
    scale_fill_gradient(low="white", high="black") +
    facet_grid(.~label) +
    geom_vline(xintercept = 14.5) +
    geom_hline(yintercept = 14.5)

data("mnist_27")
mnist_27$train %>% ggplot(aes(x_1, x_2, color = y)) + geom_point()

is <- mnist_27$index_train[c(which.min(mnist_27$train$x_2),        
                             which.max(mnist_27$train$x_2))]
titles <- c("smallest","largest")
tmp <- lapply(1:2, function(i){
    expand.grid(Row=1:28, Column=1:28) %>%
        mutate(label=titles[i],
               value = mnist$train$images[is[i],])
})
tmp <- Reduce(rbind, tmp)
tmp %>% ggplot(aes(Row, Column, fill=value)) +
    geom_raster() +
    scale_y_reverse() +
    scale_fill_gradient(low="white", high="black") +
    facet_grid(.~label) +
    geom_vline(xintercept = 14.5) +
    geom_hline(yintercept = 14.5)

fit_glm <- glm(y ~ x_1 + x_2, data=mnist_27$train, family = "binomial")
p_hat_glm <- predict(fit_glm, mnist_27$test)
y_hat_glm <- factor(ifelse(p_hat_glm > 0.5, 7, 2))
confusionMatrix(data = y_hat_glm, reference = mnist_27$test$y)$overall["Accuracy"]

mnist_27$true_p %>% ggplot(aes(x_1, x_2, fill=p)) +
    geom_raster()
mnist_27$true_p %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
    geom_raster() +
    scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
    stat_contour(breaks=c(0.5), color="black") 

p_hat <- predict(fit, newdata = mnist_27$true_p)
mnist_27$true_p %>%
    mutate(p_hat = p_hat) %>%
    ggplot(aes(x_1, x_2,  z=p_hat, fill=p_hat)) +
    geom_raster() +
    scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
    stat_contour(breaks=c(0.5),color="black") 

p_hat <- predict(fit, newdata = mnist_27$true_p)
mnist_27$true_p %>%
    mutate(p_hat = p_hat) %>%
    ggplot() +
    stat_contour(aes(x_1, x_2, z=p_hat), breaks=c(0.5), color="black") +
    geom_point(mapping = aes(x_1, x_2, color=y), data = mnist_27$test)
```

#### 3.1.7 Comprehension Check: Logistic Regression

Define a dataset using the following code:

```{r}
set.seed(2, sample.kind="Rounding")
#set.seed(2, sample.kind="Rounding") #if you are using R 3.6 or later
make_data <- function(n = 1000, p = 0.5, 
				mu_0 = 0, mu_1 = 2, 
				sigma_0 = 1,  sigma_1 = 1){

y <- rbinom(n, 1, p)
f_0 <- rnorm(n, mu_0, sigma_0)
f_1 <- rnorm(n, mu_1, sigma_1)
x <- ifelse(y == 1, f_1, f_0)
  
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)

list(train = data.frame(x = x, y = as.factor(y)) %>% slice(-test_index),
	test = data.frame(x = x, y = as.factor(y)) %>% slice(test_index))
}
dat <- make_data()
```

Note that we have defined a variable x that is predictive of a binary outcome y: 
```{r}
dat$train %>% ggplot(aes(x, color = y)) + geom_density()
```

Set the seed to 1, then use the make_data function defined above to generate 25 different datasets with mu_1 <- seq(0, 3, len=25). Perform logistic regression on each of the 25 different datasets (predict 1 if p>0.5) and plot accuracy (res in the figures) vs mu_1 (delta in the figures).

```{r}
set.seed(1, sample.kind="Rounding")

# generate 25 datasets
delta <- seq(0, 3, len=25)

# perform the process 25 tims
res <- sapply(delta, function(d){

  # create the dataset
  dat <- make_data(mu_1=d)

  # perform logistic regression fit
  glm_fit <- dat$train %>% 
    glm(y ~ x, data=., family = "binomial")
  
  # predict
  # NB!!! new data is test including xs and ys!!
  p_hat_logit <- predict(glm_fit, newdata = dat$test, type = "response")
  y_hat_logit <- ifelse(p_hat_logit > 0.5, 1, 0) %>% factor(levels = c(0, 1))
  mean(y_hat_logit == dat$test$y)
})

# plot the accuracy as a function of delta (mu)
qplot(delta, res)
```



## 3.2 Smoothing

### 3.2.1 Introduction to Smoothing

**Key points**

* **Smoothing** is a very powerful technique used all across data analysis. It is designed to **detect trend*s** in the presence of noisy data in cases in which the shape of the trend is unknown. 
* The concepts behind smoothing techniques are extremely useful in machine learning because conditional expectations/probabilities can be thought of as **trends** of unknown shapes that we need to estimate in the presence of uncertainty.

**Code**

```{r}
data("polls_2008")
qplot(day, margin, data = polls_2008)
```


### 3.2.2 Bin Smoothing and Kernels

**Key points**

The general idea of smoothing is to **group data points into strata** in which the value of $f(x)$ can be assumed to be constant. We can make this assumption because we think $f(x)$ changes slowly and, as a result, $f(x)$ is almost constant in small windows of time. 
This assumption implies that a good estimate for $f(x)$ is the average of the {Y_i} values in the window. The estimate is:

$$\hat{f}(x_0) = \frac{1}{N_0}\sum_{i\in A_0}Y_i$$
In smoothing, we call the size of the interval $\mid x-x_0\mid$ satisfying the particular condition the **window size**, **bandwidth** or **span**.

**Code**
```{r}
# bin smoothers: avging with continuity by piece
span <- 7 # 7days per week: smoothing over 1 week window
fit <- with(polls_2008, ksmooth(day, margin, x.points = day, kernel="box", bandwidth =span))
polls_2008 %>% mutate(smooth = fit$y) %>%
    ggplot(aes(day, margin)) +
    geom_point(size = 3, alpha = .5, color = "grey") + 
    geom_line(aes(day, smooth), color="red")

# kernel: avging with normal function
span <- 7   # 7days per week: smoothing over 1 week window
fit <- with(polls_2008, ksmooth(day, margin,  x.points = day, kernel="normal", bandwidth = span))
polls_2008 %>% mutate(smooth = fit$y) %>%
  ggplot(aes(day, margin)) +
  geom_point(size = 3, alpha = .5, color = "grey") + 
  geom_line(aes(day, smooth), color="red")
```


### 3.2.3 Local Weighted Regression (loess)

**Key points**

* A limitation of the bin smoothing approach is that we need small windows for the approximately constant assumptions to hold which may lead to imprecise estimates of $f(x)$ . **Local weighted regression (loess)** permits us to consider larger window sizes.
* One important difference between loess and bin smoother is that we assume **the smooth function is locally linear** in a window instead of constant.
* The result of **loess is a smoother fit than bin smoothing** because we use larger sample sizes to estimate our local parameters.

**Code**

```{r}
polls_2008 %>% ggplot(aes(day, margin)) +
  geom_point() + 
  geom_smooth(color="red", span = 0.15, method = 'loess', method.args = list(degree=1))

# NB: geom_smooth defaults to method = 'loess', polynomial approx degree =2, and span = 0.25 (proportion of points to consider for loess weighting)
```

### 3.2.4 Comprehension Check: Smoothing

#### Question 1
In the Wrangling course of this series, PH125.6x, we used the following code to obtain mortality counts for Puerto Rico for 2015-2018:

```{r}
library(tidyverse)
library(lubridate)
library(purrr)
library(pdftools)
    
fn <- system.file("extdata", "RD-Mortality-Report_2015-18-180531.pdf", package="dslabs")
dat <- map_df(str_split(pdf_text(fn), "\n"), function(s){
	s <- str_trim(s)
	header_index <- str_which(s, "2015")[1]
	tmp <- str_split(s[header_index], "\\s+", simplify = TRUE)
	month <- tmp[1]
	header <- tmp[-1]
	tail_index  <- str_which(s, "Total")
	n <- str_count(s, "\\d+")
	out <- c(1:header_index, which(n==1), which(n>=28), tail_index:length(s))
	s[-out] %>%
		str_remove_all("[^\\d\\s]") %>%
		str_trim() %>%
		str_split_fixed("\\s+", n = 6) %>%
		.[,1:5] %>%
		as_data_frame() %>% 
		setNames(c("day", header)) %>%
		mutate(month = month,
			day = as.numeric(day)) %>%
		gather(year, deaths, -c(day, month)) %>%
		mutate(deaths = as.numeric(deaths))
}) %>%
	mutate(month = recode(month, "JAN" = 1, "FEB" = 2, "MAR" = 3, "APR" = 4, "MAY" = 5, "JUN" = 6, 
                          "JUL" = 7, "AGO" = 8, "SEP" = 9, "OCT" = 10, "NOV" = 11, "DEC" = 12)) %>%
	mutate(date = make_date(year, month, day)) %>%
      filter(date <= "2018-05-01")

# Q: Use the loess function to obtain a smooth estimate of the expected number of deaths as a function of date. Plot this resulting smooth function. Make the span about two months long.

# span = 2 months / # date points
span <- 60 / as.numeric(diff(range(dat$date)))

# calculate the loess model fit
fit <- dat %>% mutate(x = as.numeric(date)) %>% loess(deaths ~ x, data = ., span = span, degree = 1)

# predict the loess model output, and plots the line curve on this output
## NB: all this can be done in one shot using ggplot2 geom_smooth() function
dat %>% mutate(smooth = predict(fit, as.numeric(date))) %>%
	ggplot() +
	geom_point(aes(date, deaths)) +
	geom_line(aes(date, smooth), lwd = 2, col = 2)
```

#### Question 2
Work with the same data as in Q1 to plot smooth estimates against day of the year, all on the same plot, but with different colors for each year.

Which code produces the desired plot?
```{r}
dat %>% 
	mutate(smooth = predict(fit, as.numeric(date)), day = yday(date), year = as.character(year(date))) %>%
	ggplot(aes(day, smooth, col = year)) +
	geom_line(lwd = 2)
```

#### Question 3
Suppose we want to predict 2s and 7s in the mnist_27 dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power.

In fact, if we fit a regular logistic regression the coefficient for x_2 is not significant!

This can be seen using this code:
```{r}
library(broom)
mnist_27$train %>% glm(y ~ x_2, family = "binomial", data = .) %>% tidy()
```

Plotting a scatterplot here is not useful since y is binary:
```{r}
qplot(x_2, y, data = mnist_27$train)
```

Fit a loess line to the data above and plot the results. What do you observe?
```{r}
mnist_27$train %>% 
	mutate(y = ifelse(y=="7", 1, 0)) %>%
	ggplot(aes(x_2, y)) + 
	geom_smooth(method = "loess")
```


## 3.3 Working with Matrices

### 3.3.1 Matrices

**Key points**

* The main reason for using matrices is that certain mathematical operations needed to develop efficient code can be performed using techniques from a branch of mathematics called **linear algebra**.
* **Linear algebra** and **matrix notation** are key elements of the language used in academic papers describing machine learning techniques. 

**Code**
```{r}
library(tidyverse)
library(dslabs)
if(!exists("mnist")) mnist <- read_mnist()

class(mnist$train$images)
x <- mnist$train$images[1:1000,] 
y <- mnist$train$labels[1:1000]
```


### 3.3.2 Matrix Notation

**Key points**

* In matrix algebra, we have three main types of objects: **scalars**, **vectors**, and **matrices**.
  + **Scalar:** $\alpha = 1$
  + **Vector:** $X_1 = \begin{pmatrix} x_{1,1} \\ \vdots \\ x_{N,1} \end{pmatrix}$
  + **Matrix:** $X = \left[ X_1 X_2 \right] = \begin{pmatrix} x_{1,1} & x_{1,2} \\ \vdots & \vdots \\ x_{N,1} & x_{N,2} \end{pmatrix}$
* In R, we can extract the dimension of a matrix with the function dim. We can convert a vector into a matrix using the function as.matrix.

**Code**
```{r}
length(x[,1])
x_1 <- 1:5
x_2 <- 6:10
cbind(x_1, x_2)
dim(x)
dim(x_1)    # nb: In R, dim requires 2D data input, dim(vector) = NULL: vectors have no dimension
dim(as.matrix(x_1)) # sol: use as.matrix(vector) conversion, then use dim()
dim(x)
```


### 3.3.3 Converting a Vector to a Matrix

**Key points**

* In R, we can **convert a vector into a matrix** with the **matrix** function. The matrix is filled in by column, but we can fill by row by using the byrow argument. The function **t** can be used to directly transpose a matrix. 

Note that the matrix function **recycles values in the vector** without warning if the product of columns and rows does not match the length of the vector.

**Code**
```{r}
my_vector <- 1:15

# fill the matrix by column
mat <- matrix(my_vector, 5, 3)
mat

# fill by row
mat_t <- matrix(my_vector, 3, 5, byrow = TRUE)
mat_t
identical(t(mat), mat_t) # checking both matrices are same

# matrix function recycles values in the vector without warning if the product of columns and rows does not match the length
matrix(my_vector, 5, 5)

# load the 3rd image into a matrix 28x28
grid <- matrix(x[3,], 28, 28)

# show the matrix using function image
# NB: image function shows the top pixel at the bottom (image must be flipped)
image(1:28, 1:28, grid)

# flip the image back
image(1:28, 1:28, grid[, 28:1])
```

### 3.3.4 Row and Column Summaries and Apply

**Key points**

* The function **rowSums()** (resp. **colSums**) computes the sum of each row.
* The function **rowMeans()** (resp. **colSums**)  computes the average of each row.
* The **matrixStats** package adds functions that performs operations on each row or column very efficiently, including the functions **rowSds** (resp. **colSds**) for rows stddev calculation.
* The function **apply(matrix, dimension, function)** lets you apply any function to a matrix: arg#1 = matrix, arg#2 = **dimension** (1 if rows, 2 if columns), and arg#3 = **function**. 

**Code**
```{r}
sums <- rowSums(x)
avg <- rowMeans(x)

data_frame(labels = as.factor(y), row_averages = avg) %>%
    qplot(labels, row_averages, data = ., geom = "boxplot")

avgs <- apply(x, 1, mean)   # same as rowMeans, but slower than the dedicated function
sds <- apply(x, 2, sd)  # same as colSds, but slower than the dedicated function
```



### 3.3.5 Filtering Columns Based on Summaries

**Key points**

* operations used to **extract columns**: x[,c(351,352)].
* operations used to **extract rows**: x[c(2,3),].
* We can also use logical indexes to determine which columns or rows to keep:  new_x <- x[ ,colSds(x) > 60].
* **Important note**: if you select only one column or only one row, the result is **no longer a matrix but a vector**. We can **preserve the matrix class** by using the argument **drop=FALSE**. 

**Code**
```{r}
library(matrixStats)

sds <- colSds(x)
qplot(sds, bins = "30", color = I("black"))
image(1:28, 1:28, matrix(sds, 28, 28)[, 28:1])

# extract columns and rows
head(x[ ,c(351,352)])
x[c(2,3),c(351,352)]
new_x <- x[, colSds(x) > 60]
dim(new_x)

# preserve the matrix class
class(x[,1])    # vector
dim(x[1,])      # dim() returns NULL

class(x[ , 1, drop=FALSE])  # keeps matrix class
dim(x[, 1, drop=FALSE])     # dim() works fine
```


### 3.3.6 Indexing with Matrices and Binarizing the Data

**Key points**

* We can use logical operations with matrices: 

        mat <- matrix(1:15, 5, 3)
        mat[mat > 6 & mat < 12] <- 0  

* We can also binarize the data using just matrix operations:

        bin_x <- x 
        bin_x[bin_x < 255/2] <- 0 
        bin_x[bin_x > 255/2] <- 1  

**Code**
```{r}
# index with matrices
mat <- matrix(1:15, 5, 3)
as.vector(mat)  # conversion matrix to vector
qplot(as.vector(x), bins = 30, color = I("black"))
new_x <- x
new_x[new_x < 50] <- 0

# set low intensity pixels at 0
mat <- matrix(1:15, 5, 3)
mat[mat < 3] <- 0
mat

mat <- matrix(1:15, 5, 3)
mat[mat > 6 & mat < 12] <- 0
mat

# binarize the data
bin_x <- x
bin_x[bin_x < 255/2] <- 0
bin_x[bin_x > 255/2] <- 1
bin_X <- (x > 255/2)*1  # same as above, but using logical and coersion
```


### 3.3.7 Vectorization for Matrices and Matrix Algebra Operations

**Key points**

* We can scale each row of a matrix using this line of code: 

        (X - rowMeans(X)) / rowSds(X)

* To scale each column of a matrix, we use this code:  

        t(t(X) - colMeans(X))

* We can also use a function called **sweep** that works similarly to **apply**. It takes each entry of a vector and subtracts it from the corresponding row or column: sweep(matrix, row or col, stat value, function)

        X_mean_0 <- sweep(X, 2, colMeans(X), FUN='-')

* Matrix multiplication: **t(x) %\*% x**
* The cross product: **crossprod(x)**
* The inverse of a function: **solve(crossprod(x))**
* The QR decomposition: **qr(x)**

**Code**
```{r 3.3.7, results="hide"}
# scale each row of a matrix
(x - rowMeans(x)) / rowSds(x)

# scale each column
t(t(x) - colMeans(x))

# take each entry of a vector and subtracts it from the corresponding row or column
x_mean_0 <- sweep(x, 2, colMeans(x))

#divide by the standard deviation
x_mean_0 <- sweep(x, 2, colMeans(x))
x_standardized <- sweep(x_mean_0, 2, colSds(x), FUN = "/")
```

### 3.3.8 Comprehension Check: Working with Matrices

#### Question 1
Q: Which line of code correctly creates a 100 by 10 matrix of randomly generated normal numbers and assigns it to x?
A: 
```{r}
x <- matrix(rnorm(100*10), 100, 10) 
```

#### Question 2
dim(x)  # dimension of  x
nrow(x) # nb rows of x

#### Question 3
Q: Which of the following lines of code would add the scalar 1 to row 1, the scalar 2 to row 2, and so on, for the matrix x?
A: 
x <- x + seq(nrow(x))
x <- sweep(x, 1, 1:nrow(x),"+")

### Question 6
For each digit in the mnist training data, compute the proportion of pixels that are in the grey area, defined as values between 50 and 205. (To visualize this, you can make a boxplot by digit class.)

Q: What proportion of pixels are in the grey area overall, defined as values between 50 and 205?
A:
```{r}
mnist <- read_mnist()
y <- rowMeans(mnist$train$images>50 & mnist$train$images<205)
mean(y)
qplot(as.factor(mnist$train$labels), y, geom = "boxplot")
```

