---
title: 'Data Science: Machine Learning - HarvardX: PH125.8x'
author: 'Luiz Cunha'
date: '2019-08-16'
output: html_notebook
---

```{r include=FALSE}
# Set knitr options for knitting code into the report:
# - echo=TRUE: print out code (echo), though any results/output would still be displayed
# - cache=TRUE: Save results so that code blocks aren't re-run unless code changes (cache),
# _or_ a relevant earlier code block changed (autodep), but don't re-run if the
# only thing that changed was the comments (cache.comments)
# - message, warning = FALSE: Don't clutter R output with messages or warnings
# This _will_ leave error messages showing up in the knitted report
# - results="hide": hide the results/output (but here the code would still be displayed)
# - include=FALSE:  chunk is evaluated, but neither the code nor its output is displayed
# - eval=FALSE: chunk is not evaluated
knitr::opts_chunk$set(echo=TRUE,
               cache=TRUE, autodep=TRUE, cache.comments=FALSE,
               message=FALSE, warning=FALSE)
```

# Section 4: Distance, Knn, Cross Validation and Generative Models

In the **Distance, kNN, Cross Validation, and Generative Models** section, you will learn about different types of discriminative and generative approaches for machine learning algorithms.

After completing this section, you will be able to:

* Use the **k-nearest neighbors (kNN)** algorithm.
* Understand the problems of **overtraining** and **oversmoothing**.
* Use **cross-validation** to reduce the **true error** and the **apparent error**. 
* Use **generative models** such as **naive Bayes**, **quadratic discriminant analysis (qda)**, and **linear discriminant analysis (lda)** for machine learning.

This section has three parts: 

1. **Nearest Neighbors**
2. **Cross-validation**
3. **Generative Models**

## 4.1 Nearest Neighbors

###  4.1.1 Distance

**Key points**

* Most clustering and machine learning techniques rely on being able to define distance between observations, using features or predictors.
* With high dimensional data, a quick way to compute all the distances at once is to use the function **dist**, which computes the distance between each row and produces an object of class **dist**:

        d <- dist(x)

* We can also compute distances between predictors. If $N$ is the number of observations, the distance between two predictors, say 1 and 2, is:

$$dist(1,2) = \sqrt{\sum_{_i=1}^{N}(x_{i,1} - x_{i,2})^2}$$

* To compute the distance between all pairs of the 784 predictors, we can transpose the matrix first and then use **dist**:

        d <- dist(t(x))

**Code**
```{r}
library(tidyverse)
library(dslabs)
if(!exists("mnist")) mnist <- read_mnist()
set.seed(1995, sample.kind="Rounding")
ind <- which(mnist$train$labels %in% c(2,7)) %>% sample(500)

# the predictors are in x and the labels in y
x <- mnist$train$images[ind,]
y <- mnist$train$labels[ind]
y[1:3]
x_1 <- x[1,]
x_2 <- x[2,]
x_3 <- x[3,]

# distance between two numbers
sqrt(sum((x_1 - x_2)^2))
sqrt(sum((x_1 - x_3)^2))
sqrt(sum((x_2 - x_3)^2))

# compute distance using matrix algebra
sqrt(crossprod(x_1 - x_2))
sqrt(crossprod(x_1 - x_3))
sqrt(crossprod(x_2 - x_3))

# compute distance between each row
d <- dist(x)
class(d) # class is 'dist'
as.matrix(d)[1:3,1:3] # cast to matrix

# visualize these distances
image(as.matrix(d))

# order the distance by labels
image(as.matrix(d)[order(y), order(y)])

# compute distance between predictors
d <- dist(t(x))
dim(as.matrix(d))
d_492 <- as.matrix(d)[492,]
image(1:28, 1:28, matrix(d_492, 28, 28))
```



###  4.1.2 Comprehension Check: Distance

#### Question 1
Load the following dataset:
```{r}
library(dslabs)
data("tissue_gene_expression")
```

This dataset includes a matrix x((sample=189, genes=500):
```{r}
dim(tissue_gene_expression$x)
```

This matrix has the gene expression levels of 500 genes from 189 biological samples representing seven different tissues. The tissue type is stored in y:
```{r}
table(tissue_gene_expression$y)
```

Which of the following lines of code computes the Euclidean distance between each observation and stores it in the object d?
```{r}
d <- dist(tissue_gene_expression$x)
d_m <- as.matrix(d)
```

#### Question 2
Using the dataset from Q1, compare the distances between observations 1 and 2 (both cerebellum), observations 39 and 40 (both colon), and observations 73 and 74 (both endometrium).
```{r}
ind <- c(1, 2, 39, 40, 73, 74)
as.matrix(d)[ind,ind]
```

Q: Distance-wise, are samples from tissues of the same type closer to each other?
A: Yes

#### Question 3
Make a plot of all the distances using the image function to see if the pattern you observed in Q2 is general.
```{r}
image(d_m)
```


###  4.1.3 KNN

**Key points**

* **K-nearest neighbors (kNN)** estimates the conditional probabilities in a similar way to bin smoothing. However, kNN is easier to adapt to multiple dimensions.
* Using KNN, for any point $(x_1,x_2)$ for which we want an estimate of $p(x_1, x_2)$, we look for the k **nearest points** to $(x_1,x_2)$ and take an average of the 0s and 1s associated with these points. We refer to the set of points used to compute the average as the **neighborhood**. Larger values of k result in smoother estimates, while smaller values of k result in more flexible and more wiggly estimates. 
* To implement the algorithm, we can use the **knn3** function from the caret package. There are two ways to call this function:
  1. We need to specify a formula and a data frame. The formula looks like this:  $outcome \sim predictor1 + predictor2 + predictor3$. The **predict** function for knn produces a probability for each class.
  2. We can also call the function with the first argument being the matrix predictors and the second a vector of outcomes, like this:

            x <- as.matrix(mnist_27$train[,2:3])
            y <- mnist_27$train$y
            knn_fit <- knn3(x,y)

**Code**
```{r}
# benchmark algorithm: logistic regression
library(caret)
library(dslabs)
mnist <- read_mnist()

fit_glm <- glm(y~x_1+x_2, data=mnist_27$train, family="binomial") # fit
p_hat_logistic <- predict(fit_glm, mnist_27$test) # predict
y_hat_logistic <- factor(ifelse(p_hat_logistic > 0.5, 7, 2))
# result: 0.76
confusionMatrix(data = y_hat_logistic, reference = mnist_27$test$y)$overall[1]

# fit knn model
knn_fit <- knn3(y ~ ., data = mnist_27$train)
x <- as.matrix(mnist_27$train[,2:3])
y <- mnist_27$train$y
knn_fit <- knn3(x, y) # way 1
knn_fit <- knn3(y ~ ., data = mnist_27$train, k=5) # way 2
# predict
y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class") # gives outcome
# result: 0.815 > 0.76 so better than logistic regression
confusionMatrix(data = y_hat_knn, reference = mnist_27$test$y)$overall["Accuracy"]
```


###  4.1.4 Overtraining and Oversmoothing

**Key points**

* **Over-training** is the reason that we have higher accuracy in the train set compared to the test set. Over-training is at its worst when we set $k=1$. With $k=1$, the estimate for each $(x_1,x_2)$ in the training set is obtained with just the $y$ corresponding to that point. 
* When we try a larger $k$, the $k$ might be so large that it does not permit enough flexibility. We call this **over-smoothing**.
* Note that if we use the test set to pick this $k$, we should not expect the accompanying accuracy estimate to extrapolate to the real world. This is because even here **we broke a golden rule of machine learning**: we selected the $k$ using the test set. **Cross validation** also provides an estimate that takes this into account.

**Code**
```{r}
y_hat_knn <- predict(knn_fit, mnist_27$train, type = "class")
confusionMatrix(data = y_hat_knn, reference = mnist_27$train$y)$overall["Accuracy"] 
y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(data = y_hat_knn, reference = mnist_27$test$y)$overall["Accuracy"]

# fit knn with k=1
knn_fit_1 <- knn3(y ~ ., data = mnist_27$train, k = 1)
y_hat_knn_1 <- predict(knn_fit_1, mnist_27$train, type = "class")
confusionMatrix(data=y_hat_knn_1, reference=mnist_27$train$y)$overall[["Accuracy"]]

# fit knn with k=401
knn_fit_401 <- knn3(y ~ ., data = mnist_27$train, k = 401)
y_hat_knn_401 <- predict(knn_fit_401, mnist_27$test, type = "class")
confusionMatrix(data=y_hat_knn_401, reference=mnist_27$test$y)$overall["Accuracy"]

# pick the k in knn
ks <- seq(3, 251, 2)
library(purrr)
accuracy <- map_df(ks, function(k){
    fit <- knn3(y ~ ., data = mnist_27$train, k = k) 
    y_hat <- predict(fit, mnist_27$train, type = "class")
    cm_train <- confusionMatrix(data = y_hat, reference = mnist_27$train$y)
    train_error <- cm_train$overall["Accuracy"]

    y_hat <- predict(fit, mnist_27$test, type = "class")
    cm_test <- confusionMatrix(data = y_hat, reference = mnist_27$test$y)
    test_error <- cm_test$overall["Accuracy"]
    tibble(train = train_error, test = test_error)
})

# pick the k that maximizes accuracy using the estimates built on the test data
ks[which.max(accuracy$test)]
max(accuracy$test)
```


### 4.1.5 Comprehension Check: Nearest Neighbors

#### Question 1
Previously, we used logistic regression to predict sex based on height. Now we are going to use knn to do the same. Set the seed to 1, then use the caret package to partition the dslabs "heights" data into a training and test set of equal size. Use the sapply function to perform knn with k values of seq(1, 101, 3) and calculate F_1 scores.

```{r}
library(dslabs)
library(tidyverse)
library(caret)
data("heights")

set.seed(1, sample.kind="Rounding") # R 3.6

test_index <- createDataPartition(heights$sex, times=1, p=0.5, list=FALSE)
train <- heights %>% slice(-test_index)
test <- heights %>% slice(test_index)

k <- seq(1, 101, 3)
F1s <- sapply(k, function(k){
    
    # calibrate model
    fit <- knn3(sex ~ height, data = train, k = k) 
    
    # predict
    y_hat <- predict(fit, test, type = "class") %>% factor(levels = levels(test$sex))

    # calculate F1 score: via confusionMatrix
#    tidy(confusionMatrix(data = y_hat, reference = test$sex)) %>% 
#        filter(term=='f1') %>%
#        .$estimate
    # calculate F1 score: directly
    F_meas(data = y_hat, reference = test$sex)
})

plot(k, F1s)

# Q: What is the max value of F_1?
# Q: At what value of k does the max occur?
max(F1s)
k[which.max(F1s)]
```

#### Question 2
Next we will use the same gene expression example used in the Comprehension Check: Distance exercises. You can load it like this:
```{r}
library(dslabs)
library(tidyverse)
library(caret)
data("tissue_gene_expression")
```

Split the data into training and test sets, and report the accuracy you obtain. 
Try it for k = 1, 3, 5, 7, 9, 11. 
Set the seed to 1 before splitting the data.
Accuracy depending on k?

```{r}
set.seed(1, sample.kind="Rounding") # R 3.6
y <- tissue_gene_expression$y
x <- tissue_gene_expression$x

#This dataset includes a matrix x((sample=189, genes=500):
dim(tissue_gene_expression$x)

test_index <- createDataPartition(y, times=1, p=0.5, list=FALSE)
#train <- data.frame(tissue_gene_expression$x[-test_index,], y=tissue_gene_expression$y[-test_index])
#test <- data.frame(tissue_gene_expression$x[test_index,], y=tissue_gene_expression$y[test_index])

ks <- seq(1, 11, 2)
res <- sapply(ks, function(k){
    
    # calibrate model
    # fit <- knn3(y ~ ., data = train, k = k) # std way
    fit <- knn3(x[-test_index,], y[-test_index], k = k) # matrix way
    
    # predict
    #y_hat <- predict(fit, test, type = "class") %>% factor(levels = levels(test$y)) # std way
    y_hat <- predict(fit, newdata = data.frame(x=x[test_index,]), type = "class") # matrix way

    # calculate Accuracy
    #confusionMatrix(data = y_hat, reference = test$y)$overall["Accuracy"]
    mean(y_hat == y[test_index])
})

res
```


## 4.2 Cross Validation

### 4.2.1 k-fold Cross Validation

**Key points**

* For *$k$-fold cross validation*, we divide the dataset into a training set and a test set. We train our algorithm exclusively on the training set and use the test set only for evaluation purposes. 

* For each set of algorithm parameters being considered, we want an *estimate of the MSE* and then we will choose the *parameters with the smallest MSE*. In $k$-fold cross validation, we randomly split the observations into $k$ non-overlapping sets, and repeat the calculation for MSE for each of these sets. Then, we compute the average MSE and obtain an estimate of our loss. Finally, we can select the optimal parameter that minimized the MSE.

* In terms of how to select $k$ for cross validation, *larger values of $k$ are preferable but they will also take much more computational time*. For this reason, the choices of $k$ =5 and $k$ =10 are common.


### 4.2.2 Comprehension Check: Cross-validation

#### Question 1
Generate a set of random predictors and outcomes using the following code:  
```{r}
library(tidyverse)
library(caret)

set.seed(1996, sample.kind="Rounding") #if you are using R 3.6 or later
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()

x_subset <- x[ ,sample(p, 100)]
```

Because x and y are completely independent, you should not be able to predict y using x with accuracy greater than 0.5. Confirm this by running cross-validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample x_subset. Use the subset when training the model.

# Q: Which code correctly performs this cross-validation?
# A:
```{r}
fit <- train(x_subset, y, method = "glm")
fit$results
```


#### Question 2
Now, instead of using a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the $y=1 $group to those in the $y=0$ group, for each predictor, using a t-test. You can do perform this step like this:

```{r}
#install.packages("BiocManager")
#BiocManager::install("genefilter")
#library(genefilter)
tt <- colttests(x, y)
```

Q: Which of the following lines of code correctly creates a vector of the p-values called pvals?  
A: pvals <- tt$p.value

#### Question 3
Create an index ind with the column numbers of the predictors that were "statistically significantly" associated with y. Use a p-value cutoff of 0.01 to define "statistically significantly."

How many predictors survive this cutoff?
```{r}
pvals <- tt$p.value
ind <- which(pvals <= 0.01)
length(ind)
```

#### Question 4
Now re-run the cross-validation after redefinining x_subset to be the subset of x defined by the columns showing "statistically significant" association with y.

What is the accuracy now?
```{r}
x_subset <- x[,ind]
fit_ <- train(x=x_subset, y, method = "glm")
fit_$results['Accuracy']
```

#### Question 5
Re-run the cross-validation again, but this time using kNN. Try out the following grid k = seq(101, 301, 25) of tuning parameters. Make a plot of the resulting accuracies.

Which code is correct?
```{r}
k <- seq(101, 301, 25)
fit5 <- train(x=x_subset, y, method = "knn", tuneGrid = data.frame(k))
ggplot(fit5)
```

#### Question 6
In the previous exercises, we see that despite the fact that x and y are completely independent, we were able to predict y with accuracy higher than 70%. We must be doing something wrong then.

Q: What is it?
A: We used the entire dataset to select the columns used in the model. 

**Explanation:**  
Because we used the entire dataset to select the columns in the model, the accuracy is too high. The selection step needs to be included as part of the cross-validation algorithm, and then the cross-validation itself is performed after the column selection step.

#### Question 7
Use the train function with kNN to select the best k for predicting tissue from gene expression on the tissue_gene_expression dataset from dslabs. Try k = seq(1,7,2) for tuning parameters. For this question, do not split the data into test and train sets (understand this can lead to overfitting, but ignore this for now).
```{r}
library(dslabs)
library(tidyverse)
library(caret)
data("tissue_gene_expression")

y <- tissue_gene_expression$y
x <- tissue_gene_expression$x

k <- seq(1, 7, 2)
fit7 <- train(x, y, method = "knn", tuneGrid = data.frame(k))
ggplot(fit7)
```

Q: What value of k results in the highest accuracy?  
A: k=1


### 4.2.3 Bootstrap

**Key points**

* When we don't have access to the entire population, we can use **bootstrap** to estimate the population median $m$.
* The bootstrap permits us to approximate a **Monte Carlo simulation** without access to the entire distribution. The general idea is relatively simple. We act as if the observed sample is the population. We then sample datasets (with replacement) of the same sample size as the original dataset. Then we compute the summary statistic, in this case the median, on this bootstrap sample.
* Note that we can use ideas similar to those used in the bootstrap in **cross validation**: instead of dividing the data into equal partitions, we simply bootstrap many times.

**Code**
```{r}
n <- 10^6
income <- 10^(rnorm(n, log10(45000), log10(3)))
qplot(log10(income), bins = 30, color = I("black"))

m <- median(income)
m

set.seed(1995)
#use set.seed(1995, sample.kind="Rounding") instead if using R 3.6 or later
N <- 250
X <- sample(income, N)
M<- median(X)
M

library(gridExtra)
B <- 10^4
M <- replicate(B, {
    X <- sample(income, N)
    median(X)
})
p1 <- qplot(M, bins = 30, color = I("black"))
p2 <- qplot(sample = scale(M)) + geom_abline()
grid.arrange(p1, p2, ncol = 2)

mean(M)
sd(M)

B <- 10^4
M_star <- replicate(B, {
    X_star <- sample(X, N, replace = TRUE)
    median(X_star)
})

tibble(monte_carlo = sort(M), bootstrap = sort(M_star)) %>%
    qplot(monte_carlo, bootstrap, data = .) + 
    geom_abline()

quantile(M, c(0.05, 0.95))
quantile(M_star, c(0.05, 0.95))

median(X) + 1.96 * sd(X) / sqrt(N) * c(-1, 1)

mean(M) + 1.96 * sd(M) * c(-1,1)

mean(M_star) + 1.96 * sd(M_star) * c(-1, 1)
```


### 4.2.4 Comprehension Check: Bootstrap

#### Question 1
The *createResample function* can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the mnist_27 dataset like this:

```{r}
set.seed(1995, sample.kind="Rounding")
indexes <- createResample(mnist_27$train$y, 10)

#summary(mnist_27$train$y)
#str(indexes)
# How many times do 3, 4, and 7 appear in the first resampled index?
sum(indexes$Resample01 == 3)
sum(indexes$Resample01 == 4)
sum(indexes$Resample01 == 7)

```


#### Question 2
We see that some numbers appear more than once and others appear no times. This has to be this way for each dataset to be independent. Repeat the exercise for all the resampled indexes.

```{r}
# What is the total number of times that 3 appears in all of the resampled indexes (could use sapply on indexes as well)?
sum(flatten(indexes) == 3)
```

#### Question 3
Generate a random dataset using the following code: *y <- rnorm(100, 0, 1)*  
Estimate the 75th quantile, which we know is qnorm(0.75), with the sample quantile: quantile(y, 0.75).

Set the seed to 1 and perform a Monte Carlo simulation with 10,000 repetitions, generating the random dataset and estimating the 75th quantile each time. What is the expected value and standard error of the 75th quantile?

```{r}
set.seed(1, sample.kind="Rounding")
B <- 10000
qs <- replicate(B, {
    y <- rnorm(100, 0, 1)
    quantile(y, 0.75)
})
c(theo = qnorm(0.75), sample_mean = mean(qs), sample_sd = sd(qs))
```

#### Question 4
In practice, we can't run a Monte Carlo simulation. Use the sample:
```{r}
set.seed(1, sample.kind="Rounding")
y <- rnorm(100, 0, 1)
```

Set the seed to 1 again after generating y and use 10 bootstrap samples to estimate the expected value and standard error of the 75th quantile.
```{r}
set.seed(1, sample.kind="Rounding")
dat4 <- createResample(y, 10)

res4 <- sapply(dat4, function(ind){
    quantile(y[ind], 0.75)
})
mean(res4)
sd(res4)
```


#### Question 5
Repeat the exercise from Q4 but with 10,000 bootstrap samples instead of 10. Set the seed to 1.
```{r}
set.seed(1, sample.kind="Rounding")
dat5 <- createResample(y, 10000)

res5 <- sapply(dat5, function(ind){
    quantile(y[ind], 0.75)
})
mean(res5)
sd(res5)
```

#### Question 6
When doing bootstrap sampling, the simulated samples are drawn from the empirical distribution of the original data.
F: The bootstrap is particularly useful in situations in which **a tractable variance formula does exist**.


## 4.3 Generative Models

### 4.3.1 Generative Models

**Key points**

* **Discriminative approaches** estimate the conditional probability directly and do not consider the distribution of the predictors. 
* **Generative models** are methods that model the joint distribution and $X$ (we model how the entire data, $X$ and $Y$ are generated).

### 4.3.2 Naive Bayes

**Key points**

* Bayes' rule:

$$p(x) = Pr(Y=1|X=x) = \frac{f_{X|Y=1}(X)Pr(Y=1)}{f_{X|Y=0}(X)Pr(Y=0) + f_{X|Y=1}(X)Pr(Y=1)}$$

with $f_{X|Y=1}$ and $f_{X|Y=0}$ representing the distribution functions of the predictor $X$ for the two classes $Y=1$ and $Y=0$ . 

The *Naive Bayes* approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text, such as The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman.

**Code**
```{r}
# Generating train and test set
library("caret")
data("heights")
y <- heights$height
set.seed(2)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)

# Estimating averages and standard deviations
params <- train_set %>%
 group_by(sex) %>%
 summarize(avg = mean(height), sd = sd(height))
params

# Estimating the prevalence
pi <- train_set %>% summarize(pi=mean(sex=="Female")) %>% pull(pi)
pi

# Getting an actual rule
x <- test_set$height
f0 <- dnorm(x, params$avg[2], params$sd[2])
f1 <- dnorm(x, params$avg[1], params$sd[1])
p_hat_bayes <- f1*pi / (f1*pi + f0*(1 - pi))
```

### 4.3.3 ControllingPrevalence

**Key points**

* The Naive Bayes approach includes a parameter to account for differences in prevalence $\pi=Pr(Y=1)$. If we use hats to denote the estimates, we can write $\hat{p}(x)$ as: 

$$\hat{p}(x) = Pr(Y=1|X=x) = \frac{\hat{f}_{X|Y=1}(x)\hat{\pi}}{\hat{f}_{X|Y=0}(x)(1-\hat{\pi}) + \hat{f}_{X|Y=}(x)Pr(Y=1)}$$

* The Naive Bayes approach gives us a direct way to correct the imbalance between sensitivity and specificity by simply forcing $\hat{p}(x)$ to be whatever value we want it to be in order to better **balance specificity and sensitivity**. 

**Code**
```{r}
# Computing sensitivity
y_hat_bayes <- ifelse(p_hat_bayes > 0.5, "Female", "Male")
sensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))

# Computing specificity
specificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))

# Changing the cutoff of the decision rule
p_hat_bayes_unbiased <- f1 * 0.5 / (f1 * 0.5 + f0 * (1 - 0.5))
y_hat_bayes_unbiased <- ifelse(p_hat_bayes_unbiased > 0.5, "Female", "Male")
sensitivity(data = factor(y_hat_bayes_unbiased), reference = factor(test_set$sex))
specificity(data = factor(y_hat_bayes_unbiased), reference = factor(test_set$sex))

# Draw plot
qplot(x, p_hat_bayes_unbiased, geom = "line") +
 geom_hline(yintercept = 0.5, lty = 2) +
 geom_vline(xintercept = 67, lty = 2)
```


### 4.3.4 QDA and LDA

**Key points**

* **Quadratic discriminant analysis (QDA)** is a version of Naive Bayes in which we assume that the distributions $p_{X|Y=1}(x)$ and $p_{X|Y=0}(x)$ are multivariate normal. 
QDA can work well with a few predictors, but it becomes **harder to use as the number of predictors increases**. Once the number of parameters approaches the size of our data, the method becomes impractical due to overfitting.
Forcing the assumption that all predictors share the same standard deviations and correlations, the boundary will be a line, just as with logistic regression. For this reason, we call the method **linear discriminant analysis (LDA)**.
In the case of LDA, the lack of flexibility **does not permit us to capture the non-linearity** in the true conditional probability function.

**Code**

QDA
```{r}
# Load data
data("mnist_27")

# Estimate parameters from the data
params <- mnist_27$train %>%
 group_by(y) %>%
 summarize(avg_1 = mean(x_1), avg_2 = mean(x_2),
        sd_1 = sd(x_1), sd_2 = sd(x_2),
        r = cor(x_1, x_2))

# Contour plots
mnist_27$train %>% mutate(y = factor(y)) %>%
 ggplot(aes(x_1, x_2, fill = y, color = y)) +
 geom_point(show.legend = FALSE) +
 stat_ellipse(type="norm", lwd = 1.5)

# Fit model
library(caret)
train_qda <- train(y ~., method = "qda", data = mnist_27$train)
# Obtain predictors and accuracy
y_hat <- predict(train_qda, mnist_27$test)
confusionMatrix(data = y_hat, reference = mnist_27$test$y)$overall["Accuracy"]

# Draw separate plots for 2s and 7s
mnist_27$train %>% mutate(y = factor(y)) %>%
 ggplot(aes(x_1, x_2, fill = y, color = y)) +
 geom_point(show.legend = FALSE) +
 stat_ellipse(type="norm") +
 facet_wrap(~y)
```

LDA
```{r}
params <- mnist_27$train %>%
 group_by(y) %>%
 summarize(avg_1 = mean(x_1), avg_2 = mean(x_2),
        sd_1 = sd(x_1), sd_2 = sd(x_2),
        r = cor(x_1, x_2))
params <- params %>% mutate(sd_1 = mean(sd_1), sd_2 = mean(sd_2), r = mean(r))
train_lda <- train(y ~., method = "lda", data = mnist_27$train)
y_hat <- predict(train_lda, mnist_27$test)
confusionMatrix(data = y_hat, reference = mnist_27$test$y)$overall["Accuracy"]
```


### 4.3.5 Case Study: More than Three Classes

**Key points**

* In this case study, we will briefly give a slightly more complex example: one with **3 classes instead of 2**. Then we will fit QDA, LDA, and KNN models for prediction.
* Generative models can be very powerful, but only when we are able to **successfully approximate the joint distribution** of predictors conditioned on each class.

**Code**
```{r}
if(!exists("mnist"))mnist <- read_mnist()

set.seed(3456) #use set.seed(3456, sample.kind="Rounding") in R 3.6 or later
index_127 <- sample(which(mnist$train$labels %in% c(1,2,7)), 2000)
y <- mnist$train$labels[index_127] 
x <- mnist$train$images[index_127,]
index_train <- createDataPartition(y, p=0.8, list = FALSE)

# get the quadrants
# temporary object to help figure out the quadrants
row_column <- expand.grid(row=1:28, col=1:28)
upper_left_ind <- which(row_column$col <= 14 & row_column$row <= 14)
lower_right_ind <- which(row_column$col > 14 & row_column$row > 14)

# binarize the values. Above 200 is ink, below is no ink
x <- x > 200 

# cbind proportion of pixels in upper right quadrant and proportion of pixels in lower right quadrant
x <- cbind(rowSums(x[ ,upper_left_ind])/rowSums(x),
           rowSums(x[ ,lower_right_ind])/rowSums(x)) 

train_set <- data.frame(y = factor(y[index_train]),
                     x_1 = x[index_train,1],
                     x_2 = x[index_train,2])

test_set <- data.frame(y = factor(y[-index_train]),
                    x_1 = x[-index_train,1],
                    x_2 = x[-index_train,2])

train_set %>%  ggplot(aes(x_1, x_2, color=y)) + geom_point()

train_qda <- train(y ~ ., method = "qda", data = train_set)
predict(train_qda, test_set, type = "prob") %>% head()
predict(train_qda, test_set) %>% head()
confusionMatrix(predict(train_qda, test_set), test_set$y)$table
confusionMatrix(predict(train_qda, test_set), test_set$y)$overall["Accuracy"]
train_lda <- train(y ~ ., method = "lda", data = train_set)
confusionMatrix(predict(train_lda, test_set), test_set$y)$overall["Accuracy"]
train_knn <- train(y ~ ., method = "knn", tuneGrid = data.frame(k = seq(15, 51, 2)),
    data = train_set)
confusionMatrix(predict(train_knn, test_set), test_set$y)$overall["Accuracy"]

train_set %>% mutate(y = factor(y)) %>% ggplot(aes(x_1, x_2, fill = y, color=y)) + geom_point(show.legend = FALSE) + stat_ellipse(type="norm")
```


### 4.3.6 Comprehension Check: Generative Models

#### Question 1
Create a dataset of samples from just cerebellum and hippocampus, two parts of the brain, and a predictor matrix with 10 randomly selected columns using the following code:

```{r}
library(dslabs)
library(caret)
library(broom)
data("tissue_gene_expression")
      
set.seed(1993, sample.kind="Rounding") # if using R 3.6 or later
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]

# Use the train function to estimate the accuracy of LDA. For this question, use the entire tissue_gene_expression dataset: do not split it into training and test sets (understand this can lead to overfitting).
# calibrate model
fit1 <- train(x, y, method = 'lda')

# Q: What is the accuracy?
fit1$results["Accuracy"]
```


#### Question 2
In this case, LDA fits two 10-dimensional normal distributions. Look at the fitted model by looking at the finalModel component of the result of train. Notice there is a component called means that includes the estimated means of both distributions. Plot the mean vectors against each other and determine which predictors (genes) appear to be driving the algorithm.

```{r}
t(fit1$finalModel$means) %>% data.frame() %>%
	mutate(predictor_name = rownames(.)) %>%
	ggplot(aes(cerebellum, hippocampus, label = predictor_name)) +
	geom_point() +
	geom_text() +
	geom_abline()

# Q: Which TWO genes appear to be driving the algorithm?
# A: RAB1B, OAZ2
```

#### Question 3
Repeat the exercise in Q1 with QDA.

Create a dataset of samples from just cerebellum and hippocampus, two parts of the brain, and a predictor matrix with 10 randomly selected columns using the following code:

```{r}
library(dslabs)
library(caret)
library(broom)
data("tissue_gene_expression")
      
set.seed(1993, sample.kind="Rounding") # if using R 3.6 or later
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]

# Use the train function to estimate the accuracy of QDA. For this question, use the entire tissue_gene_expression dataset: do not split it into training and test sets (understand this can lead to overfitting).
# calibrate model
fit3 <- train(x, y, method = 'qda')

# Q: What is the accuracy?
fit3$results["Accuracy"]
```

#### Question 4
Which TWO genes drive the algorithm when using QDA instead of LDA?
```{r}
t(fit3$finalModel$means) %>% data.frame() %>%
	mutate(predictor_name = rownames(.)) %>%
	ggplot(aes(cerebellum, hippocampus, label = predictor_name)) +
	geom_point() +
	geom_text() +
	geom_abline()

# Q: Which TWO genes appear to be driving the algorithm?
# A: RAB1B, OAZ2
```

#### Question 5
One thing we saw in the previous plots is that the values of the predictors correlate in both groups: some predictors are low in both groups and others high in both groups. The mean value of each predictor found in colMeans(x) is not informative or useful for prediction and often for purposes of interpretation, it is useful to center or scale each column. This can be achieved with the preProcess argument in train. Re-run LDA with preProcess = "center". Note that accuracy does not change, but it is now easier to identify the predictors that differ more between groups than based on the plot made in Q2.

```{r}
library(dslabs)
library(caret)
library(broom)
data("tissue_gene_expression")
      
set.seed(1993, sample.kind="Rounding") # if using R 3.6 or later
ind <- which(tissue_gene_expression$y %in% c("cerebellum", "hippocampus"))
y <- droplevels(tissue_gene_expression$y[ind])
x <- tissue_gene_expression$x[ind, ]
x <- x[, sample(ncol(x), 10)]

# Use the train function to estimate the accuracy of QDA. For this question, use the entire tissue_gene_expression dataset: do not split it into training and test sets (understand this can lead to overfitting).
# calibrate model
fit5 <- train(x, y, method = 'lda', preProcess = "center")

# Q: What is the accuracy?
fit5$results["Accuracy"]

# Q: Which TWO genes drive the algorithm after performing the scaling?
t(fit5$finalModel$means) %>% data.frame() %>%
	mutate(predictor_name = rownames(.)) %>%
	ggplot(aes(predictor_name, hippocampus)) +
	geom_point() +
    coord_flip()
```

You can see that it is different genes driving the algorithm now. This is because the predictor means change.

In the previous exercises we saw that both LDA and QDA approaches worked well. For further exploration of the data, you can plot the predictor values for the two genes with the largest differences between the two groups in a scatter plot to see how they appear to follow a bivariate distribution as assumed by the LDA and QDA approaches, coloring the points by the outcome, using the following code:

```{r}
d <- apply(fit5$finalModel$means, 2, diff)
ind <- order(abs(d), decreasing = TRUE)[1:2]
plot(x[, ind], col = y)
```


#### Question 6

Now we are going to increase the complexity of the challenge slightly. Repeat the LDA analysis from Q5 but using all tissue types. Use the following code to create your dataset:

```{r}
library(dslabs)      
library(caret)
data("tissue_gene_expression")
           
set.seed(1993) #set.seed(1993, sample.kind="Rounding") if using R 3.6 or later
y <- tissue_gene_expression$y
x <- tissue_gene_expression$x
x <- x[, sample(ncol(x), 10)]

fit6 <- train(x, y, method = 'lda', preProcess = "center")

# Q: What is the accuracy?
fit6$results["Accuracy"]
confusionMatrix(fit6)
```

